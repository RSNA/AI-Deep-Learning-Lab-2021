{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Train_YOLOv5_Practice_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PouriaRouzrokh/AI-Deep-Learning-Lab-2021/blob/Commit-Nov15/Train_YOLOv5_Practice_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07e01ff0-c83d-402c-95dc-2cad366e1d03"
      },
      "source": [
        "## Brain Hemorrhage Detection Model\n",
        "\n",
        "Welcome to the RSNA2021 Object detection workshop!\n",
        "\n",
        "In this notebook, we train a YOLOv5 deep learning model to detect brain hemorrhage on Head CT scans. \n",
        "\n",
        "*  For our model, we use the [Ultralytics](https://github.com/ultralytics/yolov5) implementation of YOLOv5.\n",
        "*  For our data, we use the publicly available [CQ500 Head CT-scan dataset](http://headctstudy.qure.ai/dataset) of patients with brain hemorrhage. \n",
        "*  For our labels, we use the bounding box annotation on CQ500 dataset by [Physionet](https://physionet.org/content/bhx-brain-bounding-box/1.1/).\n",
        "\n",
        "Hopefully, by reviewing this notebook you will be able to train a model that can detect hemorrhage lesions on brain CT scans, as plotted in the image below:\n",
        "<center><img src='https://drive.google.com/uc?export=download&id=1Jgl42NBR79DK62QxMXFHGdFFGQYTORB1'></center>\n",
        "\n",
        "In this notebook, you are supposed to complete a few coding blocks in different cells. This will help you to think better about the processes we need to take for training and applying our model. Don't worry, in case you did not find out the right code to put in the cells, we will provide it to you. We are all learning after all!\n"
      ],
      "id": "07e01ff0-c83d-402c-95dc-2cad366e1d03"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAKCHZoSUmAN"
      },
      "source": [
        "### Part 0: Setting up the working directory\n",
        "We start our work by installing some required libraries and cloning our DICOMs and ground truths from a GitHub repository that we have made before. In this notebook, we will train our model on 3500 DICOMs from the CQ500 dataset. Feel free to download the entire dataset and train stronger models on it after this workshop!"
      ],
      "id": "oAKCHZoSUmAN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU565R0cbDh3"
      },
      "source": [
        "# Installing required packages\n",
        "\n",
        "!pip install python-gdcm pydicom -q\n",
        "!pip uninstall PyYAML -y -q\n",
        "!pip install PyYAML==5.3.1 -q\n",
        "!pip install --upgrade scikit-learn -q\n",
        "!pip install --upgrade pillow -q"
      ],
      "id": "HU565R0cbDh3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyG9ovYYtDkO"
      },
      "source": [
        "**MAKE SURE TO RESTART YOUR RUNTIME BEFORE PROCEEDING. YOU CAN DO THIS BY CLICKING ON THE RUNTIME MENU FROM THE TOP OF THE PAGE -> RESTART RUNTIME**"
      ],
      "id": "zyG9ovYYtDkO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA6KK1za_xEp"
      },
      "source": [
        "# Removing the \"sample_data\" folder that google colab creates by default\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree('sample_data', ignore_errors=True)"
      ],
      "id": "aA6KK1za_xEp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CuAOpWK5IYa"
      },
      "source": [
        "# Importing DICOM and label files to our working directory\n",
        "%%time\n",
        "\n",
        "import os\n",
        "if not os.path.exists('RSNA2021_YOLOv5_Workshop'): # To make the cell work prperly if run multiple times. \n",
        "  !git clone https://github.com/Mayo-Radiology-Informatics-Lab/RSNA2021_YOLOv5_Workshop.git"
      ],
      "id": "3CuAOpWK5IYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HywYobnwBPr"
      },
      "source": [
        "OK, now feel free to check your working directory on the right side of the screen. You should click on the folder icon and will then see a folder named \"RSNA2021_YOLOv5_Workshop\". If you look inside that folder, you will see a folder containing our DICOM files and a CSV file containing our labels."
      ],
      "id": "6HywYobnwBPr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda2b69f-71e8-41e7-a610-eeb2221e84e1"
      },
      "source": [
        "### Part 1: Extracting CT images out of DICOMs\n",
        "YOLO works on 2-dimensional images, therefore we need to convert our DICOM files to images before we can train a model on them. For this, we first create a list of paths to all our DICOMs, then read each DICOM with Pydicom library, and finally convert the DICOM arrays to PNG images, while bringing them to the brain window."
      ],
      "id": "fda2b69f-71e8-41e7-a610-eeb2221e84e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75318a77-72ee-41f6-b0ed-fc7c666eae4c"
      },
      "source": [
        "# Collecting the paths to DICOM files\n",
        "\n",
        "import os\n",
        "\n",
        "DICOMs_dir = 'RSNA2021_YOLOv5_Workshop/DICOMs'\n",
        "dcmpaths = list()\n",
        "for root, dirs, files in os.walk(DICOMs_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.dcm'):\n",
        "            dcmpaths.append(os.path.join(root, file))\n",
        "\n",
        "print(f'{len(dcmpaths)} DICOMs were found!')"
      ],
      "id": "75318a77-72ee-41f6-b0ed-fc7c666eae4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_2VgHm6ZXOS"
      },
      "source": [
        "import pydicom\n",
        "import numpy as np\n",
        "\n",
        "def Read_DICOM(dcmpath):\n",
        "  \"\"\"\n",
        "  Recive a DICOM file, extract its image array, and convert the pixel values\n",
        "  to the Hounsfield Unit (HU). \n",
        "  inputs:\n",
        "    - dcmpath: Path to a DICOM file saved on disk.\n",
        "  outputs:\n",
        "    - img: Extracted image array from the DICOM file that has been \n",
        "           conveted to HU.\n",
        "  \"\"\"\n",
        "\n",
        "  # What you should do:\n",
        "\n",
        "  # 1- Load the DICOM file using pydicom (tip: use pydicom.dcmread command).\n",
        "\n",
        "  # 2- Extract the image array from the DICOM (tip: the image array inside a \n",
        "  # DICOM file could be accessed using dcm.pixel_array command). \n",
        "\n",
        "  # 3- Get the DICOM slope (tip: use dcm.RescaleSlope). \n",
        "\n",
        "  # 4- Get the DICOM intercept (tip: use dcm.RescaleIntercept).\n",
        "\n",
        "  # 5- Multiply each pixel value of the image array in the DICOM slope and \n",
        "  # then add the output to the DICOM intercept.\n",
        "\n",
        "  ##### START YOUR CODE HERE (~3 - 5 lines): \n",
        "\n",
        "\n",
        "  ##### END YOUR CODE HERE.\n",
        "\n",
        "  return img"
      ],
      "id": "i_2VgHm6ZXOS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wNmkC4LiZ5qY"
      },
      "source": [
        "#@title Code to complete the previous cell!\n",
        "\n",
        "# dcm = pydicom.dcmread(dcmpath)\n",
        "# img = dcm.pixel_array\n",
        "# intercept = float(dcm.RescaleIntercept)\n",
        "# slope = float(dcm.RescaleSlope)\n",
        "# img = slope * img + intercept"
      ],
      "id": "wNmkC4LiZ5qY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X16ZNuK6ZM-F"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images_dir = 'images'\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def Apply_window(img, ww=80, wl=40):\n",
        "    \"\"\"\n",
        "    A function to apply windowing on the CT-scan. Default values represent the brain window.\n",
        "    \"\"\"\n",
        "    U = 255\n",
        "    W = U / ww\n",
        "    b = (-U/ww) * (wl-ww/2)\n",
        "    img = W*img + b\n",
        "    img = np.where(img > U, U, img)\n",
        "    img = np.where(img < 0, 0, img)\n",
        "    return img"
      ],
      "id": "X16ZNuK6ZM-F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68606375-c9f4-4cf8-986a-5cce74556a8a"
      },
      "source": [
        "def Extract_image(dcmpath, test_run=False):\n",
        "    \"\"\"\n",
        "    Extract imaging array out of a given DICOM file.\n",
        "    \"\"\"\n",
        "  \n",
        "    # Reading the imaging data from the DICOM file and converting \n",
        "    # its pixel values to the Hounsfield Unit (HU).\n",
        "    img = Read_DICOM(dcmpath)\n",
        "    \n",
        "    # Widnowing to the brain window.\n",
        "    img = Apply_window(img)\n",
        "    \n",
        "    # Normalizing and changing the img to 8-bit.\n",
        "    img -= img.mean()\n",
        "    img /= (img.std() + 1e-10)\n",
        "    img -= img.min()\n",
        "    img = (255 * img/np.max(img)).astype('uint8')\n",
        "    \n",
        "    # Saving the img as a PNG file to images_dir\n",
        "    # A DICOM's name is : {SOPInstanceUID}.dcm\n",
        "    # We name the image files similiarly: {SOPInstanceUID}.png\n",
        "    \n",
        "    pil_img = Image.fromarray(img)\n",
        "    pil_img.save(f'{images_dir}/{dcmpath.split(\"/\")[-1][:-4]}.png')\n",
        "    \n",
        "    # Testing the function's performance if needed\n",
        "    if test_run:\n",
        "        plt.imshow(pil_img, cmap='gray')\n",
        "    \n",
        "# Testing the Extract_image function\n",
        "Extract_image(dcmpaths[400], test_run=True)"
      ],
      "id": "68606375-c9f4-4cf8-986a-5cce74556a8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9482ad50-1c28-453f-b7f5-d8b038c99b05"
      },
      "source": [
        "# Extracting CT images out of all DICOMs \n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for dcmpath in tqdm(dcmpaths):\n",
        "    Extract_image(dcmpath)"
      ],
      "id": "9482ad50-1c28-453f-b7f5-d8b038c99b05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4feff68-134c-4f5c-9dad-e2d012020e80"
      },
      "source": [
        "# Collecting the paths to all saved images\n",
        "\n",
        "imgpaths = [os.path.join('images', file) for file in os.listdir('images')]\n",
        "print(f'{len(imgpaths)} images were found!')"
      ],
      "id": "d4feff68-134c-4f5c-9dad-e2d012020e80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8a7x2Ciy17t"
      },
      "source": [
        "OK, now check your working directory again. You should see a folder named \"images\" in which you should find all images from our dataset."
      ],
      "id": "Z8a7x2Ciy17t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2dc5d0-c99c-4830-b04b-3ac13c34c5d1"
      },
      "source": [
        "### Part 2: Collecting and visualizing the annotated bounding box labels\n",
        "Now that we took care of the images, let's take a look at our ground truth labels. For this, we use Pandas to load our annotation CSV file."
      ],
      "id": "9f2dc5d0-c99c-4830-b04b-3ac13c34c5d1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "866e2ad2-2b0e-4c8c-86aa-499f35b00c9b"
      },
      "source": [
        "# Importing our annotation dataset as a Pandas dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "labels_csv_path = 'RSNA2021_YOLOv5_Workshop/labels.csv'\n",
        "labels_df = pd.read_csv(labels_csv_path)\n",
        "\n",
        "num_bboxes = len(labels_df)\n",
        "num_unique_images = len(pd.unique(labels_df.SOPInstanceUID))\n",
        "num_unique_scans = len(pd.unique(labels_df.SeriesInstanceUID))                    \n",
        "print (f'The dataframe includes {num_bboxes} bounding boxes from {num_unique_images} images and {num_unique_scans} scans!\\n')\n",
        "\n",
        "labels_df.head()"
      ],
      "id": "866e2ad2-2b0e-4c8c-86aa-499f35b00c9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT60SiYtzRcu"
      },
      "source": [
        "Looking at the above dataframe, please note two things:\n",
        "\n",
        "1.   Each Row in our dataframe belongs to one single **bounding box** for a hemorrhage lesion (and not a CT slice or nor a patient). So each slice of a CT scan may be on more than one row if it contains more than one hemorrhage lesion!\n",
        "2.   DICOMs are also de-identified and do not have a PatientID tag, yet they do have *StudyInstanceUID* tags. We will soon talk about this more!"
      ],
      "id": "cT60SiYtzRcu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e70ce180-60e4-4ead-907b-9fe5ac40f7e3"
      },
      "source": [
        "# Finding out the possible labels and assigning a color to each label\n",
        "\n",
        "all_possible_labels = set(labels_df.labelName.tolist())\n",
        "colors = ['red', 'blue', 'green', 'orange', 'pink', 'purple']\n",
        "label_color_dict = {label:color for label, color in zip(all_possible_labels, colors)}\n",
        "\n",
        "for label in label_color_dict.keys():\n",
        "    print(f'{label}: {label_color_dict[label]}')"
      ],
      "id": "e70ce180-60e4-4ead-907b-9fe5ac40f7e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNzMKdhx1FwB"
      },
      "source": [
        "It's a good idea to plot some images and their manually plotted bounding boxes before we start the training. This will help us have a better sense of what data we are dealing with. Feel free to run the cell below a few times and see multiple different images with their annotations."
      ],
      "id": "zNzMKdhx1FwB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa6a4ed4-a1a2-4e5e-911b-6e815cb52b0e"
      },
      "source": [
        "# Plotting random sample images along with their annotated bounding boxes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from skimage.io import imread\n",
        "import ast\n",
        "\n",
        "def Collect_GT_bboxes(imgpath):\n",
        "    \"\"\"\n",
        "    Collect ground truth (manually annotated) bounding box coordinates for a given imgpath.\n",
        "    \"\"\"\n",
        "    sop_uid = imgpath.split('/')[-1][:-4]\n",
        "    img_df = labels_df[labels_df.SOPInstanceUID==sop_uid]\n",
        "    bboxes = list()\n",
        "    for i, row in img_df.iterrows():\n",
        "        bbox = ast.literal_eval(row['data'])\n",
        "        bbox['labelName'] = row['labelName']\n",
        "        bboxes.append(bbox)\n",
        "    return bboxes    \n",
        "\n",
        "# Plot bboxes on 9 random imgpaths\n",
        "from random import choices\n",
        "sample_imgpaths = choices(imgpaths, k=9)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        imgpath = sample_imgpaths[i*3 + j]\n",
        "        img = imread(imgpath)\n",
        "        bboxes = Collect_GT_bboxes(imgpath)\n",
        "        axes[i, j].imshow(img, cmap='gray')\n",
        "        labels = list()\n",
        "        for bbox in bboxes:\n",
        "            xmin = bbox['x'] \n",
        "            ymin = bbox['y']\n",
        "            label = bbox['labelName']\n",
        "            labels.append(label)\n",
        "            rect = patches.Rectangle((xmin, ymin), bbox['width'], bbox['height'], \n",
        "                                     linewidth=1, \n",
        "                                     edgecolor=label_color_dict[label], \n",
        "                                     facecolor='none')\n",
        "            axes[i, j].add_patch(rect)\n",
        "        axes[i, j].axis('off')\n",
        "        axes[i, j].set_title('\\n'.join(labels))\n",
        "for label in label_color_dict.keys():\n",
        "    print(f'{label}: {label_color_dict[label]}')"
      ],
      "id": "fa6a4ed4-a1a2-4e5e-911b-6e815cb52b0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88410834-3a35-493b-a3dd-28bf0297fd97"
      },
      "source": [
        "### Part 3: Data splitting and setting up the training and validation files"
      ],
      "id": "88410834-3a35-493b-a3dd-28bf0297fd97"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwdZgNXp1mCN"
      },
      "source": [
        "**Ideally, we would like to split our data into different sets (training, validation, test) based on PatientIDs. Here, we need to do the split based on StudyInstanceUID, which we assume are unique for each patient (A patient may have multiple studies and a study may contain more than one scan, yet we assume that each patient in our pool has only one study)**. DICOMs also have *SeriesInstanceUID* tags that are the same for all DICOMs from a single CT scan, but different between scans, and *SOPInstanceUID* tags, which are unique for each DICOM regardless of the patient or the scan it belongs to. \n",
        "\n",
        "Saying above, let's start splitting our data to a training and a validation set. Ideally, we would like to also have a test set or endorse a k-fold cross-validation strategy, yet we keep things simple for this workshop!"
      ],
      "id": "EwdZgNXp1mCN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09752bc6-7a0b-4b48-a914-25b32e780804"
      },
      "source": [
        "# Building the folders needed for data splitting\n",
        "# These folders will be added to our working directory.\n",
        "\n",
        "train_imgs_dir = 'model_data/images/train'\n",
        "train_labels_dir = 'model_data/labels/train'\n",
        "valid_imgs_dir = 'model_data/images/valid'\n",
        "valid_labels_dir = 'model_data/labels/valid'\n",
        "\n",
        "os.makedirs(train_imgs_dir, exist_ok=True)\n",
        "os.makedirs(train_labels_dir, exist_ok=True)\n",
        "os.makedirs(valid_imgs_dir, exist_ok=True)\n",
        "os.makedirs(valid_labels_dir, exist_ok=True)"
      ],
      "id": "09752bc6-7a0b-4b48-a914-25b32e780804",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcubTtGi2WpA"
      },
      "source": [
        "Here we create three lists based on our dataframe: \n",
        "\n",
        "*   X: A list of all SOPInstanceUIDs we got from all rows (don't forget that each DICOM has a unique SOPInstanceUID, so this list includes all our DICOM names).\n",
        "*   Y: A list of all labels (or the types of hemorrhage lesions) for all rows.\n",
        "*   groups: A list of all StudyInstanceUIDs.\n",
        "\n",
        "What we need to do, is to split our X into 2 different folds (80% training and 20% validation), while controlling for the group variable (so that all rows for one specific StudyInstaceUID tag go to either training or validation sets) and also stratifying our data based on the labels (so that the distribution of different labels between training and validation sets be as close as possible to our 80-20 split)."
      ],
      "id": "UcubTtGi2WpA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgOE_tr1Mt1q"
      },
      "source": [
        "# To make the cell work prperly if run multiple times. \n",
        "try:\n",
        "  train_df \n",
        "  valid_df\n",
        "except NameError:\n",
        "  train_df = None; valid_df=None"
      ],
      "id": "UgOE_tr1Mt1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hga3Z677aHmR"
      },
      "source": [
        "# Splitting the data into training and validation sets.\n",
        "\n",
        "X = labels_df.SOPInstanceUID.tolist()\n",
        "Y = labels_df.labelName.tolist()\n",
        "groups = labels_df.StudyInstanceUID.tolist()\n",
        "\n",
        "# To make the cell work prperly if run multiple times. \n",
        "if train_df is None and valid_df is None: \n",
        "  from sklearn.model_selection import StratifiedGroupKFold\n",
        "  cv = StratifiedGroupKFold(n_splits=5)\n",
        "  train_idxs, valid_idxs = next(iter(cv.split(X, Y, groups)))\n",
        "  train_df = labels_df.loc[train_idxs]\n",
        "  valid_df = labels_df.loc[valid_idxs]\n",
        "  train_uids = set(train_df.StudyInstanceUID.tolist())\n",
        "  valid_uids = set(valid_df.StudyInstanceUID.tolist())\n",
        "\n",
        "print(f'Number of hemorrhage instances (bboxes) in the training set: {len(train_df)} - Number of patients: {len(train_uids)})')\n",
        "print(f'Class distribution in the training set: ')\n",
        "print(train_df['labelName'].value_counts())\n",
        "print('\\n***********\\n')\n",
        "print(f'Number of hemorrhage instances (bboxes) in the validation set: {len(valid_df)} - Number of patients: {len(valid_uids)})')\n",
        "print(f'Class distribution in the validation set: ')\n",
        "print(valid_df['labelName'].value_counts())\n",
        "\n",
        "# Making sure training and validation data have no patients in common\n",
        "assert len(set(train_df['StudyInstanceUID'].tolist()).intersection(set(valid_df['StudyInstanceUID'].tolist()))) == 0"
      ],
      "id": "hga3Z677aHmR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61pWUtxr5HaF"
      },
      "source": [
        "You would probably have enjoyed how easy you could do the tedious task of splitting using the StratifiedGroupKFold command of Scikit-learn! Check the numbers above and see how stratified our split is based on labels. \n",
        "\n",
        " Please note that this command is currently available in the Beta version of Scikit-learn, so we needed to uninstall the default Scikit-learn library of Google Colab and install the Beta version by our own (we did it in the first cell of this notebook). \n",
        "\n",
        "Now that we built the folds, let's proceed by actually copying our images to training and validation directories."
      ],
      "id": "61pWUtxr5HaF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25e3b704-57a3-4ffb-9cb1-da87aef05491"
      },
      "source": [
        "# Moving the images to their folders in the \"model_data\" directory\n",
        "\n",
        "for i, row in train_df.iterrows():\n",
        "    imgpath = f'images/{row.SOPInstanceUID}.png'\n",
        "    shutil.copy(imgpath, os.path.join(train_imgs_dir, f'{row.SOPInstanceUID}.png'))\n",
        "    \n",
        "for i, row in valid_df.iterrows():\n",
        "    imgpath = f'images/{row.SOPInstanceUID}.png'\n",
        "    shutil.copy(imgpath, os.path.join(valid_imgs_dir, f'{row.SOPInstanceUID}.png'))"
      ],
      "id": "25e3b704-57a3-4ffb-9cb1-da87aef05491",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8krypmQ54re"
      },
      "source": [
        "YOLO needs us two TXT files including the paths to images in our training and validation directories as well, so let's create those files:"
      ],
      "id": "N8krypmQ54re"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2efb5ef-c73c-43db-8bb8-b3b5c320ec85"
      },
      "source": [
        "# To make the cell work prperly if run multiple times. \n",
        "shutil.rmtree('model_data/train.txt', ignore_errors=True)\n",
        "shutil.rmtree('model_data/val.txt', ignore_errors=True)\n",
        "\n",
        "# Building the txt files including paths to all images in training and validation sets\n",
        "\n",
        "with open('model_data/train.txt', 'w') as f:\n",
        "    for file in os.listdir(train_imgs_dir):\n",
        "        f.write(os.path.join(train_imgs_dir, file)+'\\n')\n",
        "            \n",
        "with open('model_data/val.txt', 'w') as f:\n",
        "    for file in os.listdir(valid_imgs_dir):\n",
        "        f.write(os.path.join(valid_imgs_dir, file)+'\\n')"
      ],
      "id": "d2efb5ef-c73c-43db-8bb8-b3b5c320ec85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPFPio0i6Hk5"
      },
      "source": [
        "OK, now we should create the labels for training our YOLO model.\n",
        "YOLO needs a TXT file for each image that contains the locations of each bounding box in that image, plus the index for the class of that box in separate lines. \n",
        "\n",
        "We will create these files below. For doing this, we first give a unique index to each class of brain hemorrhage in our dataset and then write their location in files. Here is where we will need your coding again:\n",
        "\n",
        "YOLO needs four coordinates for each box:\n",
        "\n",
        "*  x_min: The x-coordinate for the upper-left corner of the box in pixels.\n",
        "*  y_min: The y-coordinate for the upper-left corner of the box in pixels.\n",
        "*  width: The wideness of the box in pixels.\n",
        "*  height: The height of the box in pixels.\n",
        "\n",
        "We should note that the way YOLO receives ground truth labels from us (and further predicts labels) is special. YOLO needs all the above numbers proportional to the size of the image, (and not in their absolute values). For example, if the image size is 512 * 512 (as in our case), and the x-min for an imaginary box is 100, YOLO expects a float number (100/512) as the x-min in our labels. This rule also applies to all other bounding box numbers YOLO needs."
      ],
      "id": "NPFPio0i6Hk5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEwS8JiXizOB"
      },
      "source": [
        "def Convert_bbox_toYOLO(bbox, image_size):\n",
        "  \"\"\"\n",
        "  Receive the coordinates for a given bbox and convert its coordinates to the \n",
        "  YOLO format.\n",
        "  \n",
        "  inputs:\n",
        "    - bbox (dict): a dictionary with the following keys: \n",
        "    (all have absolute values.)\n",
        "      -- 'x': the x coordinate for the top left point of the bounding box.\n",
        "      -- 'y': the y coordinate for the top left point of the bounding box.\n",
        "      -- 'width': the width of the bounding box.\n",
        "      -- 'height': the height of the bounding box.\n",
        "   \n",
        "    - image_size (int): the shape of the image is (image_size, image_size)\n",
        "  \n",
        "  outputs:\n",
        "    - yolo_bbox (dict): a dictionary with the following keys:\n",
        "    (all have values scaled between 0 - 1 based on the image_size.)\n",
        "      -- 'x_center': the x coordinate for the center of the bounding box.\n",
        "      -- 'y_center': the y coordinate for the center of the bounding box.\n",
        "      -- 'width': the width of the bounding box.\n",
        "      -- 'height': the height of the bounding box.\n",
        "  \"\"\"\n",
        "\n",
        "  yolo_bbox = dict()\n",
        "\n",
        "  ##### START YOUR CODE HERE (4 lines of code):\n",
        "\n",
        "  \n",
        "  ##### END YOUR CODE HERE.\n",
        "\n",
        "  return yolo_bbox"
      ],
      "id": "fEwS8JiXizOB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "K-tpWKk6moko"
      },
      "source": [
        "#@title Code to complete the previous cell!\n",
        "\n",
        "# yolo_bbox['x_center'] = (bbox['x'] + bbox['width'] / 2) / image_size\n",
        "# yolo_bbox['y_center'] = (bbox['y'] + bbox['height'] / 2) / image_size\n",
        "# yolo_bbox['width'] = bbox['width'] / image_size\n",
        "# yolo_bbox['height'] = bbox['height'] / image_size"
      ],
      "id": "K-tpWKk6moko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52560851-198f-49a7-b118-5ff97f27c88f"
      },
      "source": [
        "# To make the cell work prperly if run multiple times. \n",
        "shutil.rmtree(train_labels_dir, ignore_errors=True) \n",
        "os.makedirs(train_labels_dir, exist_ok=True)\n",
        "shutil.rmtree(valid_labels_dir, ignore_errors=True)\n",
        "os.makedirs(valid_labels_dir, exist_ok=True)\n",
        "\n",
        "# Creating the labels\n",
        "\n",
        "label_to_index_dict = {\n",
        "    'Chronic': 0,\n",
        "    'Intraventricular': 1,\n",
        "    'Subdural': 2,\n",
        "    'Intraparenchymal': 3,\n",
        "    'Subarachnoid': 4,\n",
        "    'Epidural': 5\n",
        "}\n",
        "\n",
        "# Creating the TXT file for each image\n",
        "# Each image will have a single TXT file including all its labels (each file \n",
        "# may have multiple lines, each for one bounding box on that image).\n",
        "\n",
        "for img_file in os.listdir(train_imgs_dir):\n",
        "    bboxes = Collect_GT_bboxes(f'images/{img_file}')\n",
        "    label_file = img_file[:-4] + '.txt'\n",
        "    with open(f'{train_labels_dir}/{label_file}', 'w') as f:\n",
        "        for bbox in bboxes:\n",
        "            label = str(label_to_index_dict[bbox['labelName']])\n",
        "            yolo_bbox = Convert_bbox_toYOLO(bbox, image_size=512)\n",
        "            x_center = yolo_bbox['x_center']\n",
        "            y_center = yolo_bbox['y_center']\n",
        "            width = yolo_bbox['width']\n",
        "            height = yolo_bbox['height']\n",
        "            line_to_write = ' '.join([label, str(x_center), \n",
        "                                      str(y_center), str(width), str(height)])\n",
        "            f.write(line_to_write)\n",
        "            f.write('\\n')\n",
        "            \n",
        "for img_file in os.listdir(valid_imgs_dir):\n",
        "    bboxes = Collect_GT_bboxes(f'images/{img_file}')\n",
        "    label_file = img_file[:-4] + '.txt'\n",
        "    with open(f'{valid_labels_dir}/{label_file}', 'w') as f:\n",
        "        for bbox in bboxes:\n",
        "            # Simply copy your code from the block above.\n",
        "            label = str(label_to_index_dict[bbox['labelName']])\n",
        "            yolo_bbox = Convert_bbox_toYOLO(bbox, image_size=512)\n",
        "            x_center = yolo_bbox['x_center']\n",
        "            y_center = yolo_bbox['y_center']\n",
        "            width = yolo_bbox['width']\n",
        "            height = yolo_bbox['height']\n",
        "            line_to_write = ' '.join([label, str(x_center), \n",
        "                                      str(y_center), str(width), str(height)])\n",
        "            f.write(line_to_write)\n",
        "            f.write('\\n')"
      ],
      "id": "52560851-198f-49a7-b118-5ff97f27c88f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cd38bfc-52d5-4fc7-9a99-d90561412cfb"
      },
      "source": [
        "### Part 4: Downloading the YOLO model and configuring it\n",
        "Alright, now that we have all the images and labels set up, we can clone the Ultralytics YOLOv5 repository and configure that for our training."
      ],
      "id": "5cd38bfc-52d5-4fc7-9a99-d90561412cfb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e67bd781-6079-4c99-888a-7626a0a6bc41"
      },
      "source": [
        "# Clonning the YOLOv5 directory from Ultralytics GitHub page\n",
        "\n",
        "model_dir = 'yolov5'\n",
        "\n",
        "# To make the cell work prperly if run multiple times. \n",
        "shutil.rmtree(model_dir, ignore_errors=True)\n",
        "\n",
        "!git clone https://github.com/ultralytics/yolov5  {model_dir} # clone repo"
      ],
      "id": "e67bd781-6079-4c99-888a-7626a0a6bc41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn3kuuF58791"
      },
      "source": [
        "As the first step in our configuration, we need to change the YAML file in the model directory. We should give it the path to our training and validation directories of images, the number of classes, and the name of classes."
      ],
      "id": "pn3kuuF58791"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c25a0b07-4ec7-4dd2-813b-934b3ad4cf3a"
      },
      "source": [
        "# Configuring the data.yaml file\n",
        "\n",
        "import yaml\n",
        "\n",
        "data = dict(\n",
        "    train = train_imgs_dir,\n",
        "    val = valid_imgs_dir,   \n",
        "    nc    = 6, # number of classes\n",
        "    names = list(label_to_index_dict.keys()) # classes\n",
        "    )\n",
        "\n",
        "with open(f'{model_dir}/data/data.yaml', 'w') as file:\n",
        "    yaml.dump(data, file, default_flow_style=False)\n",
        "\n",
        "with open(f'{model_dir}/data/data.yaml', 'r') as file:\n",
        "    for line in file.readlines():\n",
        "        print(line.strip())"
      ],
      "id": "c25a0b07-4ec7-4dd2-813b-934b3ad4cf3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59750870-ba3c-48f4-90a2-1b7a361cdb97"
      },
      "source": [
        "### Part 5: View and modify the hyperparameters (optional)\n",
        "The Ultralytics implementation of YOLO gives us the ability to change many of the hyperparameters YOLO works with. These are all accessible in a YAML file in the model's directory. We will not touch these settings for now, but let's visualize them before we proceed:"
      ],
      "id": "59750870-ba3c-48f4-90a2-1b7a361cdb97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162c5d86-9ba8-408e-8383-a3047d4e1dad"
      },
      "source": [
        "with open(f'{model_dir}/data/hyps/hyp.scratch.yaml', 'r') as file:\n",
        "    for line in file.readlines():\n",
        "        print(line.strip())"
      ],
      "id": "162c5d86-9ba8-408e-8383-a3047d4e1dad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb6a7344-20b6-48cf-96c2-83838bf32c64"
      },
      "source": [
        "### Part 6: Training\n",
        "Perfect, now everything is set for us to start the training! Fortunately, the training itself could be run using one line of code! We just need to determine the image size, batch size, number of epochs, the directory to our model's directory, a name for our project, and a name for our current run of experiment. \n",
        "\n",
        "Please note that the training command should be run from the command line, that is why we have put an \"!\" mark before the line we do the training.\n",
        "\n",
        "**Note: In Google Colab, we cannot run the training with a batch size greater than 8, otherwise we will hit the memory limits. Training with such a small batch size on the other hand may take a lot of time, so we only train our model for one epoch. Please run this notebook locally or give it more time later for a full training.**"
      ],
      "id": "eb6a7344-20b6-48cf-96c2-83838bf32c64"
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "15eaff63-d161-411b-97de-053533d81b8e"
      },
      "source": [
        "# Training Hyperparameters\n",
        "%%time\n",
        "\n",
        "IMAGE_SIZE = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "WEIGHTS_PATH = f'{model_dir}/yolov5x.pt'\n",
        "PROJECT_dir = f'{model_dir}/RSNA_YOLO_Project'\n",
        "RUN = 'Exp1'\n",
        "\n",
        "# To make the cell work prperly if run multiple times. \n",
        "shutil.rmtree(os.path.join(PROJECT_dir, RUN), ignore_errors=True)\n",
        "\n",
        "# Training\n",
        "!python  {model_dir}/train.py   --img {IMAGE_SIZE} \\\n",
        "                                --batch {BATCH_SIZE} \\\n",
        "                                --epochs {EPOCHS} \\\n",
        "                                --data  {model_dir}/data/data.yaml \\\n",
        "                                --weights {WEIGHTS_PATH} \\\n",
        "                                --project  {PROJECT_dir}\\\n",
        "                                --name {RUN} \\"
      ],
      "id": "15eaff63-d161-411b-97de-053533d81b8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyWmWE4P54wT"
      },
      "source": [
        "As the training will probably take longer than the time we have available in this workshop, we will work with a pre-trained model from now on. We have already trained this model using almost the same configuration we set earlier.You can download that model and replace it in a usual place in our working directory by running the following cell:"
      ],
      "id": "UyWmWE4P54wT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rX_G6ysmRs9"
      },
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?export=download&id=1rpMzYyna1N3bcZ-_ZmEVr9IhA8FDpjTH'\n",
        "output = 'Pretrained_YOLO.zip'\n",
        "if not os.path.exists(output): # To make the cell work prperly if run multiple times. \n",
        "  gdown.download(url, output, quiet=False)"
      ],
      "id": "4rX_G6ysmRs9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JhTkgkunDu4"
      },
      "source": [
        "# Let's replace the downloaded model with the initial model we were training.\n",
        "shutil.rmtree(f'{PROJECT_dir}/{RUN}', ignore_errors=True)\n",
        "!unzip Pretrained_YOLO.zip -d {PROJECT_dir}"
      ],
      "id": "6JhTkgkunDu4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB-767wg_GD-"
      },
      "source": [
        "All right, now we assume that we have done a full training. The Ultralytics implementation of YOLOv5 plots a lot of useful curves and logs much information during the training. You could easily visualize that inforation by looking at your model_dir/project/run directory or even using loggers like TensorBoard or WandB."
      ],
      "id": "EB-767wg_GD-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b16ba308-25d1-4552-a98a-3a06a2a06c11"
      },
      "source": [
        "# Visualizing the training performance\n",
        "\n",
        "def Show_performance(run_name:str, project_dir:str=PROJECT_dir):\n",
        "    \n",
        "    run_path = os.path.join(project_dir, run_name)\n",
        "    \n",
        "    # Model's performance\n",
        "    results_df = pd.read_csv(os.path.join(run_path, 'results.csv'))\n",
        "    metrics = {'precision':[], 'recall':[], 'mAP.05':[]}\n",
        "    for i, row in results_df.iterrows():\n",
        "        for metric, key in zip(['precision', 'recall', 'mAP.05'], ['   metrics/precision', '      metrics/recall', '     metrics/mAP_0.5']):\n",
        "            metrics[metric].append(row[key])   \n",
        "    max_precision = max(metrics['precision'])\n",
        "    max_recall = max(metrics['recall'])\n",
        "    max_mAP = max(metrics['mAP.05'])\n",
        "    print(f\"Best precision: Epoch {metrics['precision'].index(max_precision)} -> {max_precision}\")\n",
        "    print(f\"Best recall: Epoch {metrics['recall'].index(max_recall)} -> {max_recall}\")\n",
        "    print(f\"Best mAP.05: Epoch {metrics['mAP.05'].index(max_mAP)} -> {max_mAP}\")\n",
        "    \n",
        "    # Training curves\n",
        "    print(\"\\nDisplaying the training curves:\")\n",
        "    plt.figure(figsize = (12,12))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(plt.imread(os.path.join(run_path, 'results.png')));\n",
        "    plt.show()\n",
        "    \n",
        "    #GTs vs predictions\n",
        "    print(\"\\nDisplaying the ground truths vs predictions for three example batches from the validation set:\")\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 20))\n",
        "    for i in range(3):\n",
        "        axes[i, 0].imshow(plt.imread(os.path.join(run_path, f'val_batch{i}_labels.jpg')))\n",
        "        axes[i, 1].imshow(plt.imread(os.path.join(run_path, f'val_batch{i}_pred.jpg')))\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 1].axis('off')\n",
        "        axes[i, 0].set_title('Ground Truths')\n",
        "        axes[i, 1].set_title('Predictions')"
      ],
      "id": "b16ba308-25d1-4552-a98a-3a06a2a06c11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c04bfaa-b298-4379-9a22-3a665d390eec"
      },
      "source": [
        "Show_performance(RUN)"
      ],
      "id": "3c04bfaa-b298-4379-9a22-3a665d390eec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6N6Td_CFx_m"
      },
      "source": [
        "**Please note that the best mAP of our model has been about 0.4. Though not very bad, this mAP is not very high. Rather than the training hyperparameters, What other factors do you think can explain this phenomenon?**"
      ],
      "id": "P6N6Td_CFx_m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQpA826WwNe4"
      },
      "source": [
        "### Part 7: Inference\n",
        "As the final part of our notebook, let's apply our YOLO model to a set of images. For doing this, we separate a part of images that our model had not seen during the training (Please note that these images are different than the validation images you created above, as we are now working with a model that we had trained before; not a model that you trained here). "
      ],
      "id": "sQpA826WwNe4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hDzxmbq_0vp"
      },
      "source": [
        "# Seperating a part of our images which our pre-trained model has not seen for training\n",
        "\n",
        "inference_imgs_dir = 'Inference_imgs'\n",
        "os.makedirs(inference_imgs_dir, exist_ok=True)\n",
        "\n",
        "with open(f'{PROJECT_dir}/{RUN}/images_for_inference.txt', 'r') as f:\n",
        "  inference_img_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Copying 20 images for inference to the inference_imgs_dir\n",
        "\n",
        "count_copied = 0\n",
        "for image in os.listdir('images'):\n",
        "  if image in inference_img_names:\n",
        "    shutil.copy(f'images/{image}', f'{inference_imgs_dir}/{image}')\n",
        "    count_copied += 1\n",
        "  if count_copied == 20:\n",
        "    break"
      ],
      "id": "5hDzxmbq_0vp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bje-WKxMLsap"
      },
      "source": [
        "# Setting up the pipeline for inference\n",
        "\n",
        "weights_path = f'{PROJECT_dir}/{RUN}/weights/best.pt'\n",
        "destination_dir = 'Inference_Results'\n",
        "output_name = 'Pretrained_YOLOv5_Results'\n",
        "img_size = 512\n",
        "conf: float = 0.222\n",
        "iou_threshold: float = 0.5\n",
        "max_dt = 5"
      ],
      "id": "Bje-WKxMLsap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQjQDTNM7FF"
      },
      "source": [
        "# We should now run the detect.py code from the model directory and pass all \n",
        "# the arguments that YOLO needs. \n",
        "\n",
        "!python {model_dir}/detect.py  --weights {weights_path} \\\n",
        "                               --source {inference_imgs_dir} \\\n",
        "                               --project {destination_dir} \\\n",
        "                               --name {output_name}\\\n",
        "                               --img {img_size} \\\n",
        "                               --conf {conf} \\\n",
        "                               --iou-thres {iou_threshold} \\\n",
        "                               --max-det {max_dt} \\\n",
        "                               --save-txt \\\n",
        "                               --save-conf \\\n",
        "                               --exist-ok"
      ],
      "id": "qmQjQDTNM7FF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRnaqsBXB0DD"
      },
      "source": [
        "Before we wrap up this notebook, we can also visualize some of the YOLO predictions!"
      ],
      "id": "zRnaqsBXB0DD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKzxo1ehttp9"
      },
      "source": [
        "def Show_prediction(img_path):\n",
        "  \"\"\"\n",
        "  A function to plot an image and show the bounding boxes predicted by YOLO for that image.\n",
        "  \"\"\"\n",
        "  index_to_label_dict = {value: key for key, value in label_to_index_dict.items()}\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "  ax.imshow(imread(img_path), cmap='gray')\n",
        "  label_path = f'{destination_dir}/{output_name}/labels/{img_path.split(\"/\")[-1][:-4]}.txt'\n",
        "  with open(label_path, 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        line = [float(value) for value in line.strip().split(' ')]\n",
        "        label = index_to_label_dict[int(line[0])]\n",
        "        color = label_color_dict[label]\n",
        "        confidence = round(float(line[-1]), 2)\n",
        "        x_min = (line[1] - line[3]/2) * 512\n",
        "        y_min = (line[2] - line[4]/2) * 512\n",
        "        width = line[3] * 512\n",
        "        height = line[4] * 512 \n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(5, 15+i*25, f'{label}-confidence: {confidence}', color='w', fontsize=10, bbox={'alpha':0.6,'color':color})\n",
        "\n",
        "# Now we build a list of all paths to images we had split for validaiton of our model\n",
        "# And then feed them to the function we defined above.\n",
        "# Feel free to visualize the predictions for multple images and see how the results \n",
        "# look like!\n",
        "img_paths = [os.path.join(inference_imgs_dir, img) for img in os.listdir(inference_imgs_dir)]\n",
        "Show_prediction(img_paths[0])"
      ],
      "id": "NKzxo1ehttp9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JYz5QJFb7h"
      },
      "source": [
        "That's it! Congratulations on training and applying your first YOLOv5 model. Hopefully, you have learned how to train more YOLO models on your custom datasets. In case you are interested, there are a few more things you can try with YOLOv5:\n",
        "\n",
        "*   Changing the hyperparameters (e.g., IOU, learning rate, etc.) and retraining your model.\n",
        "*   Applying test time augmentation (TTA) during your inference.\n",
        "*   Exploring the different augmentations YOLO does during the training, especially the mosaic augmentation. \n",
        "\n",
        "\n",
        "\n",
        "**Thank you for attending this workshop!**"
      ],
      "id": "t3JYz5QJFb7h"
    }
  ]
}