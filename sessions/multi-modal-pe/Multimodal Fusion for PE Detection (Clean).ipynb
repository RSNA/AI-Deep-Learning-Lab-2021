{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/marshuang80/AI-Deep-Learning-Lab-2021/blob/multimodal-pe/sessions/multi-modal-pe/Multimodal%20Fusion%20for%20PE%20Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSdKnAJl0Urr"
   },
   "source": [
    "# Multimodal Fusion for Pulmonary Embolism Classification\n",
    "\n",
    "> by **Mars (Shih-Cheng) Huang**\n",
    "\n",
    "> email: *mschuang@stanford.edu*\n",
    "\n",
    "In this demonstration, we will recreate the results from our manuscript *Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case-study in pulmonary embolism detection*. \n",
    "\n",
    "Specifically, we will build a multimodal fusion model (late fusion) that combines information from both CT scans and Electronic Medical Record (EMR) to automatically diagnose the presence/absence of PE. \n",
    "\n",
    "![Workflow](https://github.com/marshuang80/AI-Deep-Learning-Lab-2021/blob/multimodal-pe/sessions/multi-modal-pe/figs/workflow.png?raw=1)\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**Clinical Motivation** \n",
    "\n",
    "Pulmonary Embolism (PE) is a serious medical condition† that hospitalizes 300,000 people in the United States every year. The gold standard diagnostic modality for PE is Computed Tomography Pulmonary Angiography (CTPA) which is interpreted by radiologists. Studies have shown that prompt diagnosis and treatment can greatly reduce morbidity and mortality. Strategies to automate accurate interpretation and timely reporting of CTPA examinations may successfully triage urgent cases of PE to the immediate attention of physicians, improving time to diagnosis and treatment.\n",
    "\n",
    "**Technical Motivation** \n",
    "\n",
    "Recent advancements in deep learning have led to a resurgence of medical imaging and Electronic Medical Record (EMR) models for a variety of applications, including clinical decision support, automated workflow triage, clinical prediction and more. However, very few models have been developed to integrate both clinical and imaging data, despite that in routine practice clinicians rely on EMR to provide context in medical imaging interpretation.\n",
    "\n",
    "\n",
    "\n",
    "### Fusion Strategies\n",
    "![Fusion Strategies](https://github.com/marshuang80/AI-Deep-Learning-Lab-2021/blob/multimodal-pe/sessions/multi-modal-pe/figs/fusion_strategies.png?raw=1)\n",
    "\n",
    "### Data\n",
    "We will use a subset of RadFusion, a large-scale multimodal pulmonary embolism detection dataset consisting of 1837 CT imaging studies (comprising 600,000+ 2D CT slices) for 1794 patients and their corresponding EHR summary data. The full dataset with CT scans can be access via the following link: \n",
    "- https://stanfordaimi.azurewebsites.net/datasets/3a7548a4-8f65-4ab7-85fa-3d68c9efc1bd\n",
    "\n",
    "### References\n",
    "- Huang, Shih-Cheng, et al. \"PENet—a scalable deep-learning model for automated diagnosis of pulmonary embolism using volumetric CT imaging.\" NPJ digital medicine 3.1 (2020): 1-9.\n",
    "- Huang, Shih-Cheng, et al. \"Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case-study in pulmonary embolism detection.\" Scientific reports 10.1 (2020): 1-9.\n",
    "- Zhou, Yuyin, et al. \"RadFusion: Benchmarking Performance and Fairness for Multimodal Pulmonary Embolism Detection from CT and EHR.\" arXiv preprint arXiv:2111.11665 (2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYqQxeKF0Urv"
   },
   "source": [
    "## Research Use Agreement\n",
    "\n",
    "Before we can proceed to download the data, please agree to this **Research Use Agreement** by registering to download from our website:\n",
    "- https://stanfordaimi.azurewebsites.net/datasets/3a7548a4-8f65-4ab7-85fa-3d68c9efc1bd\n",
    "\n",
    "\n",
    "![User Agreement](https://github.com/marshuang80/AI-Deep-Learning-Lab-2021/blob/multimodal-pe/sessions/multi-modal-pe/figs/UserAgreement.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuxRbEun0Urw"
   },
   "source": [
    "## System Setup & Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6KU6ap80Urx",
    "outputId": "8ff3429e-8f1a-4d40-c3e5-f8f028039c38"
   },
   "outputs": [],
   "source": [
    "!pip install numpy pandas scikit-learn matplotlib\n",
    "!gdown --id 1w0ocK3br8oqVwn6zK5qgtRaj9Ql37dtd  # /content/Demographics.csv\n",
    "!gdown --id 1MEhVZ87J2IwFmkgxOi8WjdVKTdwOpDDY  # /content/INP_MED.csv\n",
    "!gdown --id 1PRgFvQjqEUudeJ0FLR3DbtvqmI7t7sCT  # /content/OUT_MED.csv\n",
    "!gdown --id 1EDZOYmWrvv6D3XaZrjVous95c9HdiBEx  # /content/Vitals.csv\n",
    "!gdown --id 1Nlm1ZgibRv6kJBIJkQHkRh8oPqUpELnK  # /content/ICD.csv\n",
    "!gdown --id 17Y9DJsolaRPyMkk_Xm3w-iCgSOxkQOyf  # /content/LABS.csv\n",
    "!gdown --id 1JDb5f18uNo2hXXQqcHlRbcjswph1y98h  # /content/Vision.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAQw3sno0Ury"
   },
   "source": [
    "## Data Exploration\n",
    "After downloading the data, you should be able to find the following files in your directory: \n",
    "    \n",
    "- Demographics.csv \n",
    "- INP_MED.csv\n",
    "- OUT_MED.csv\n",
    "- Vitals.csv\n",
    "- ICD.csv\n",
    "- LABS.csv\n",
    "- Vision.csv\n",
    "\n",
    "Let's explore the contents in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqjy8qL_6ikl",
    "outputId": "bb8c0760-2d0a-44db-a831-a4aee41a1aea"
   },
   "outputs": [],
   "source": [
    "! ls /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR9jz5H20Ury"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsKPP3Op0Urz"
   },
   "source": [
    "### Patient Demographics\n",
    "\n",
    "The demographic features consist of one-hot encoded gender, race and smoking habits and the age as a numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "_KbDiSBT0Urz",
    "outputId": "e84fd2da-800a-4adb-f28c-f57851d5f10e"
   },
   "outputs": [],
   "source": [
    "demo_df = pd.read_csv('/content/Demographics.csv')\n",
    "print(demo_df.shape)\n",
    "demo_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy2xfJdb0Ur0"
   },
   "source": [
    "### Inpatient & Outpatient Medications\n",
    "\n",
    "641 unique classes of drugs were identified for inpatient & outpatient medication. Each medication was represented as both the frequency within the 12-month window and a binary label of whether the drug was prescribed to the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "5NRR6u550Ur0",
    "outputId": "bc87e231-1c6c-43c7-847b-1b535538c846"
   },
   "outputs": [],
   "source": [
    "out_med_df = pd.read_csv('/content/OUT_MED.csv')\n",
    "print(out_med_df.shape)\n",
    "out_med_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "ihUT8aoV0Ur1",
    "outputId": "8e4ab7b7-5a79-418e-9e76-fa15c6e197b7"
   },
   "outputs": [],
   "source": [
    "in_med_df = pd.read_csv('/content/INP_MED.csv')\n",
    "print(in_med_df.shape)\n",
    "in_med_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1AuWQPm0Ur1"
   },
   "source": [
    "### ICD Codes\n",
    "\n",
    "We excluded all ICD codes with less than 1% occurrences in the training dataset and collapsed into top diagnosis categories, which resulted in a total of 141 diagnosis groups. We used a binary presence/absence as well as a frequency to represent diagnosis code as features. All ICD codes recorded with the same encounter number as the patient’s CT exam, or within a 24 hour window prior to their CT examination, were dropped to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "Q1KbnDsv0Ur1",
    "outputId": "7ac3d9d9-c151-41ce-ffae-076505353940"
   },
   "outputs": [],
   "source": [
    "icd_df = pd.read_csv('/content/ICD.csv')\n",
    "print(icd_df.shape)\n",
    "icd_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hDUoeBi0Ur2"
   },
   "source": [
    "### Lab Tests\n",
    "\n",
    "We identified 22 lab tests and represented each test as binary presence/absence as well as the latest value of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "jptLjEkm0Ur2",
    "outputId": "60068c37-e98a-4e64-e47e-363def12e9eb"
   },
   "outputs": [],
   "source": [
    "lab_df = pd.read_csv('/content/LABS.csv')\n",
    "print(lab_df.shape)\n",
    "lab_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7O3srjL0Ur2"
   },
   "source": [
    "### Vitals\n",
    "\n",
    "For vitals, we included systolic and diastolic blood pressure, height, weight, body mass index (BMI), temperature, respiration rate, pulse oximetry (spO2) and heart rate. The vitals were represented with respect to their sensitivity to change, which was computed by taking the derivative of the vital values along the temporal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4lT3evdg0Ur3",
    "outputId": "d3400417-705f-4772-cc26-943dcb81442f"
   },
   "outputs": [],
   "source": [
    "vitals_df = pd.read_csv('/content/Vitals.csv')\n",
    "vitals_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFk0oavw0Ur3"
   },
   "source": [
    "### CTs Scans\n",
    "\n",
    "The RadFusion dataset includes CTPA scans for each study. Due to time and computational constraint, we have ran inference on these CT scans using PENet, and stored the prediction probabilities in **Vision.csv**. Additional, this csv file incldues the labels (PE positive / PE negative), the type of PE (central, segmental and sub-segmental) and the train/val/test split used to develope PENet. For more information about PENet, please refer to: \n",
    "- Manuscript: [https://www.nature.com/articles/s41746-020-0266-y](https://www.nature.com/articles/s41746-020-0266-y)\n",
    "- GitHub: [https://github.com/marshuang80/penet](https://github.com/marshuang80/penet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "_NdD0AH30Ur3",
    "outputId": "18c1217b-b2ad-4ab9-9365-09561e1f9d27"
   },
   "outputs": [],
   "source": [
    "# TODO, remove pe_type if label = 0\n",
    "vision_df = pd.read_csv('/content/Vision.csv')\n",
    "vision_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2fjy6NY0Ur3"
   },
   "source": [
    "## Process Data\n",
    "\n",
    "We are going to pre-process the EMR data by: \n",
    "- Remove any features with zero variance \n",
    "- Normalize all features to be within the same range\n",
    "\n",
    "Next, we are going to combine all the EMR features into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "O8ahgXou0Ur4",
    "outputId": "9c3891ec-f2eb-441d-a50c-1e12d2d80738"
   },
   "outputs": [],
   "source": [
    "processed_emr_dfs = []\n",
    "for df in [demo_df, out_med_df, in_med_df, icd_df, lab_df, vitals_df]:\n",
    "    # remove zero variance featurs\n",
    "    df = df.loc[:,df.apply(pd.Series.nunique) != 1]\n",
    "    \n",
    "    # set index \n",
    "    df = df.set_index('idx')\n",
    "\n",
    "    # normalize features\n",
    "    df = df.apply(lambda x: (x - x.mean())/(x.std()))\n",
    "    \n",
    "    processed_emr_dfs.append(df)\n",
    "\n",
    "emr_df = pd.concat(processed_emr_dfs, axis=1)\n",
    "emr_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKXCAv4b1npr"
   },
   "source": [
    "## Create Data Splits\n",
    "\n",
    "Next, we are going to create training, validation and test splits to develop our models. We want to make sure that we use the same data splits as the vision model (PENet), so we will join our EMR dataframe with the vision dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKmtXn2B0Ur4"
   },
   "outputs": [],
   "source": [
    "# Define columns\n",
    "EMR_FEATURE_COLS = emr_df.columns.tolist()\n",
    "PE_TYPE_COL = 'pe_type'\n",
    "SPLIT_COL = 'split'\n",
    "VISION_PRED_COL = 'pred'\n",
    "EMR_PRED_COL = 'emr_pred'\n",
    "FUSION_PRED_COL = 'late_fusion_pred'\n",
    "LABEL_COL = 'label'\n",
    "\n",
    "# Join vision information with emr dataframe\n",
    "vision_df = vision_df.set_index('idx')\n",
    "df = pd.concat([vision_df, emr_df], axis=1)\n",
    "\n",
    "# Create data splits\n",
    "df_dev = df[(df[SPLIT_COL] == 'train') | (df[SPLIT_COL] == 'val')]  # for gridsearch CV\n",
    "df_train = df[df[SPLIT_COL] == 'train']\n",
    "df_val = df[df[SPLIT_COL] == 'val']\n",
    "df_test = df[df[SPLIT_COL] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGksQMbU0Ur5"
   },
   "source": [
    "## Train EMR Model\n",
    "For our EMR data, we are going to train a simple logistic regression model.\n",
    "\n",
    "In particular we are going to train the logistic regression model with the elasticnet penalty, which linearly combines the L₁ and L₂ penalties of the lasso and ridge methods.\n",
    "\n",
    "We will use the **LogisticRegression** class from sklearn for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJUCkviY0Ur5",
    "outputId": "c4759d46-384d-4bcc-85a1-1b6cd05872e8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Uncomment and run grid search if time permits\n",
    "\"\"\"\n",
    "# define model\n",
    "clf = LogisticRegression(\n",
    "    penalty='elasticnet', solver='saga', random_state=0\n",
    ")\n",
    "\n",
    "# define grid search\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1.0, 100], \n",
    "    \"max_iter\": [10, 100, 1000],\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "}\n",
    "gsc = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# run grid search\n",
    "gsc.fit(df_dev[EMR_FEATURE_COLS], df_dev[LABEL_COL])\n",
    "print(f\"Best parameters: {gsc.best_params_}\")\n",
    "clf = gsc.best_estimator_\n",
    "\"\"\"\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    penalty='elasticnet', \n",
    "    solver='saga', \n",
    "    random_state=0,\n",
    "    C= 0.1, \n",
    "    class_weight='balanced', \n",
    "    l1_ratio= 0.99, \n",
    "    max_iter= 1000\n",
    ")\n",
    "clf.fit(df_train[EMR_FEATURE_COLS], df_train[LABEL_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTtxKD1d0Ur5"
   },
   "source": [
    "## Test EMR Model\n",
    "\n",
    "Using the trained EMR model, we will run inference on our held-out test set and extract the prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVMQeDfC0Ur5"
   },
   "outputs": [],
   "source": [
    "# test with best model\n",
    "emr_prob = clf.predict_proba(df_test[EMR_FEATURE_COLS])\n",
    "\n",
    "# take probability of positive class \n",
    "emr_prob = [p[1] for p in emr_prob]\n",
    "\n",
    "df_test = df_test.assign(emr_pred = emr_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrDhhMHK0Ur5"
   },
   "source": [
    "## Late Fusion (Mean Aggregation)\n",
    "\n",
    "<img src=\"https://github.com/marshuang80/AI-Deep-Learning-Lab-2021/blob/multimodal-pe/sessions/multi-modal-pe/figs/late_fusion_mean_agg.png?raw=1\" width=\"200\">\n",
    "\n",
    "Now that we are have prediction probabilities from both the EMR and Vision model, we will apply a simple late fusion strategy with mean aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOsWfC4P0Ur5"
   },
   "outputs": [],
   "source": [
    "# Late fusion by taking the average prediction probability from vision model and emr model\n",
    "late_fusion_pred = np.mean(\n",
    "    [df_test[EMR_PRED_COL], df_test[VISION_PRED_COL]], \n",
    "    axis=0\n",
    ")\n",
    "df_test = df_test.assign(late_fusion_pred = late_fusion_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-ag0_5Z0Ur6"
   },
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XSiBXsbN0Ur6",
    "outputId": "c475b3c5-c842-4a3b-da92-9e50e90b6566"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(7, 7))\n",
    "lw = 2\n",
    "\n",
    "def plot_auc(df, label):\n",
    "    # PENet performance\n",
    "    fpr_v, tpr_v, _ = metrics.roc_curve(\n",
    "        df[LABEL_COL], \n",
    "        df[VISION_PRED_COL])\n",
    "    roc_auc_v = metrics.auc(fpr_v, tpr_v)\n",
    "    plt.plot(\n",
    "        fpr_v, \n",
    "        tpr_v, \n",
    "        color='darkorange',\n",
    "        lw=lw, \n",
    "        label='PENet ROC curve (area = %0.2f)' % roc_auc_v)\n",
    "\n",
    "    # EMR model performance\n",
    "    fpr_emr, tpr_emr, _ = metrics.roc_curve(\n",
    "        df[LABEL_COL], \n",
    "        df[EMR_PRED_COL])\n",
    "    roc_auc_emr = metrics.auc(fpr_emr, tpr_emr)\n",
    "    plt.plot(\n",
    "        fpr_emr, \n",
    "        tpr_emr,\n",
    "        lw=lw, \n",
    "        label='EMR Model ROC curve (area = %0.2f)' % roc_auc_emr)\n",
    "\n",
    "    # Fusion model performance\n",
    "    fpr_fusion, tpr_fusion, _ = metrics.roc_curve(\n",
    "        df[LABEL_COL], \n",
    "        df[FUSION_PRED_COL])\n",
    "    roc_auc_fusion = metrics.auc(fpr_fusion, tpr_fusion)\n",
    "    plt.plot(\n",
    "        fpr_fusion, \n",
    "        tpr_fusion,\n",
    "        lw=lw, \n",
    "        label='Fusion Model ROC curve (area = %0.2f)' % roc_auc_fusion)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 0.95])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.axes().set_aspect('equal', 'datalim')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver operating characteristic ({label})')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "fEMRmB-b0Ur6",
    "outputId": "e88009f5-e6a2-4533-9036-3ad19b273d9f"
   },
   "outputs": [],
   "source": [
    "# Performance for all cases\n",
    "plot_auc(df_test, 'All Cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "DpFHeBhC0Ur6",
    "outputId": "281f25bc-f7f9-440b-a37c-82bb46e36e9c"
   },
   "outputs": [],
   "source": [
    "# Performance for non-subsegmental cases\n",
    "df_test_no_subseg = df_test[\n",
    "    df_test[PE_TYPE_COL] != 'subsegmental']\n",
    "plot_auc(df_test_no_subseg, 'No Subsegmental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "VAG9blrx0Ur6",
    "outputId": "c045e41d-a243-4b97-f6f5-61ae02bf35bf"
   },
   "outputs": [],
   "source": [
    "# Visualize histogram of Predicted Probs\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# style\n",
    "plt.clf()\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rc('xtick', labelsize=5) \n",
    "matplotlib.rc('ytick', labelsize=5) \n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(7,3), dpi=150)\n",
    "bins = np.linspace(0, 1, 30)\n",
    "\n",
    "# seperate cases into positive and negative\n",
    "positive_cases = df_test_no_subseg[\n",
    "    df_test_no_subseg[LABEL_COL] == 1]\n",
    "negative_cases = df_test_no_subseg[\n",
    "    df_test_no_subseg[LABEL_COL] == 0]\n",
    "\n",
    "# PENet\n",
    "ax1.hist(\n",
    "    [positive_cases[VISION_PRED_COL], negative_cases[VISION_PRED_COL]], \n",
    "    bins, \n",
    "    label=['positive','negative'], \n",
    "    width=0.01)\n",
    "\n",
    "# EMR\n",
    "ax2.hist(\n",
    "    [positive_cases[EMR_PRED_COL], negative_cases[EMR_PRED_COL]], \n",
    "    bins, \n",
    "    label=['positive', 'negative'], \n",
    "    width=0.01)\n",
    "\n",
    "# Fusion\n",
    "ax3.hist(\n",
    "    [positive_cases[FUSION_PRED_COL], negative_cases[FUSION_PRED_COL]], \n",
    "    bins, \n",
    "    label=['positive','negative'], \n",
    "    width=0.01)\n",
    "\n",
    "f.tight_layout(pad=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "ax2.set_xlabel(\"Predicted Probabilities\", fontsize = 10)\n",
    "ax1.set_ylabel(\"Count\", fontsize = 10)\n",
    "ax1.set_title('Vision Only', fontsize = 10)\n",
    "ax2.set_title('EMR Only', fontsize = 10)\n",
    "ax3.set_title('Fusion', fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRHKH9cC0Ur6"
   },
   "source": [
    "# Bonus: Other Fusion Strategies\n",
    "\n",
    "![OtherFusionStrategies](https://github.com/marshuang80/AI-Deep-Learning-Lab-2021/blob/multimodal-pe/sessions/multi-modal-pe/figs/other_fusion_strategies.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5qcPYsQ0Ur7"
   },
   "outputs": [],
   "source": [
    "# Try out other fusion strategies here\n",
    "\n",
    "## Option 1: Use another classifier for our EMR model (SVC, Decision Tree, Neural Networks...) \n",
    "## Option 2: Use another aggregator for late fusion (Max, Meta-classifier)\n",
    "## Option 3: Train separate classfiers for each type of EMR data before fusion (ICD, Vitals, Demographics ...)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Multimodal Fusion for PE Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitec9bac52ca3c411ebc0b7adf9e9ef198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
