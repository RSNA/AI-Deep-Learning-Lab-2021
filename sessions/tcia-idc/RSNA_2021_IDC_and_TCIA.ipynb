{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RSNA 2021: Analyzing data from TCIA and IDC",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "hxJzr-GDIlAV"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedorov/AI-Deep-Learning-Lab-2021/blob/idc-tcia/sessions/tcia-idc/RSNA_2021_IDC_and_TCIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy9vfOYmSgiz"
      },
      "source": [
        "# RSNA 2021: Working with public datasets: TCIA and IDC\n",
        "\n",
        "The goal of this session is to introduce you to the two data repositories supported by the US National Cancer Institute:\n",
        "\n",
        "* The Cancer Imaging Archive (TCIA)\n",
        "* Imaging Data Commons (IDC), which is the imaging repository within NCI Cancer Research Data Commons (CRDC)\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. Understand basic capabilities of TCIA and IDC, and the differences between the two repositories.\n",
        "2. Explore relevant functionality of TCIA and IDC to support data exploration, cohort definition, and retrieval of the data.\n",
        "3. Learn how to analyze the data retrieved from TCIA/IDC on an example of a lung nodule segmentation task.\n",
        "\n",
        "This notebook will guide you thought the complete process of identifying a relevant dataset, retrieving it, preparing it for processing by the specific analysis tool, installing the tool and applying it to the dataset, and visualizing the segmentation results produced by the tool.\n",
        "\n",
        "Note that it is not the purpose of this tutorial to promote a specific tool, or assess its robustness. \n",
        "\n",
        "We aim to provide an example of how a tool can be used for analyzing a sample dataset from TCIA/IDC. We hope that after completing this tutorial you will be empowered and motivated to experiment with more tools and apply them to more datasets in TCIA/IDC!\n",
        "\n",
        "**Session Authors**\n",
        "\n",
        "* Andrey Fedorov\n",
        "* Justin Kirby\n",
        "* Dennis Bontempi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY1nlTbvZg2b"
      },
      "source": [
        "## Outline\n",
        "\n",
        "There is a growing number of tools being developed for analyzing medical images. More and more of those are shared openly by the authors to support dissemination of findings and reproducibility of scientific studies. \n",
        "\n",
        "However, getting such tools to work is not always straightforward. Some of the challenges include deployment of the tool, identifying datasets that are suitable for analysis using a specific tool, preprocessing of the data.\n",
        "\n",
        "In this notebook we will guide you through the process of deploying one such tool on a Colab VM, and demonstrate how you can utilize publicly available repositories of cancer imaging data to find relevant datasets, how to preprocess them for analysis by a specific tool, and how to visualize results of image segmentation produced by the tool. \n",
        "\n",
        "In this tutorial we will work with the nnU-Net segmentation tool developed by Isensee et al in the following publication:\n",
        "\n",
        "> Isensee, F., Jaeger, P. F., Kohl, S. A. A., Petersen, J. & Maier-Hein, K. H. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat. Methods 18, 203–211 (2021).\n",
        "\n",
        "and shared in https://github.com/MIC-DKFZ/nnUNet.\n",
        "\n",
        "Specifically, we will utilize the network pretrained to segment 4 abdominal organs at risk (AORs): heart, aorta, trachea and esophagus. The network was trained using the [SegTHOR (Segmentation of THoracic Organs at Risk) dataset](https://arxiv.org/abs/1912.05950) and is shared in this Zenodo entry (see [Task055_SegTHOR.zip](https://zenodo.org/record/4485926/files/Task055_SegTHOR.zip?download=1)):\n",
        "\n",
        "> Isensee, Fabian, Jäger, Paul F., Kohl, Simon A. A., Petersen, Jens, & Maier-Hein, Klaus H. (2021). pretrained models for 3D semantic image segmentation with nnU-Net (2.1). Zenodo. https://doi.org/10.5281/zenodo.4485926\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMDzWEcX90lX"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "* To use Colab, and to access data in IDC, you will need a [Google Account](https://support.google.com/accounts/answer/27441?hl=en)\n",
        "* Make sure your Colab instance has a GPU! For this check \"Runtime > Change runtime type\" and make sure to choose the GPU runtime.\n",
        "* To perform queries against IDC, and to download imaging data from Google Storage buckets you will need a cloud project with configured billing. You can apply for an IDC-sponsored cloud project using [this form](https://docs.google.com/forms/d/e/1FAIpQLSfXvXqficGaVEalJI3ym6rKqarmW_YUUWG6A4U8pclvR8MmRQ/viewform). \n",
        "  * **NOTE: if you are working with this notebook at RSNA 2021, please mention \"RSNA 2021\" in the comments section of the application form! This way we will onboard you to a shared sponsored project we set up specifically for the conference. Applications submitted during the RSNA 2021 tutorial session will be processed rapidly!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGobegw-Bvz5"
      },
      "source": [
        "## nnU-Net model setup\n",
        "\n",
        "**Download of the pretrained network is the most time-consuming steps in this tutorial, so please execute the cells in this section before anything else!**\n",
        "\n",
        "While the nnU-Net framework should take care of the model download (from Zenodo), some of the zip files containing the pre-trained weights are particularly large, so the download can take a lot of time, get stuck, or produce errors (as [reported by other users](https://github.com/MIC-DKFZ/nnUNet/issues/358#issue-726410474) and in the [repository FAQ](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/common_problems_and_solutions.md#downloading-pretrained-models-unzip-cannot-find-zipfile-directory-in-one-of-homeisenseennunetdownload_16031094034174126)) .\n",
        "\n",
        "For this reason, and for the purpose of speeding up this tutorial, we decided to copy the relevant model weights in a shared Dropbox folder. In the following cells, we use Linux `wget` to pull such files from the folder - and use the nnU-Net framework command `nnUNet_install_pretrained_model_from_zip` to unpack and install the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sipNQgeB7fo"
      },
      "source": [
        "# create the directory tree\n",
        "!mkdir -p tutorial \n",
        "!mkdir -p tutorial/models tutorial/data tutorial/output\n",
        "!mkdir -p tutorial/data/dicom tutorial/data/processed tutorial/data/nnUNet_raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzPWnwbqB-Bt"
      },
      "source": [
        "# this will usually take between one and five minutes (but can sometimes take up to eight)\n",
        "seg_model_url = \"https://www.dropbox.com/s/m7es2ojn8h0ybhv/Task055_SegTHOR.zip?dl=0\"\n",
        "output_path = \"tutorial/models/Task055_SegTHOR.zip\"\n",
        "\n",
        "!wget -O $output_path $seg_model_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62yhEkkjITzn"
      },
      "source": [
        "Unpack and install model (under `PATH_TO_MODEL_FILE`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOVsPuzxIOXX"
      },
      "source": [
        "%%capture\n",
        "!nnUNet_install_pretrained_model_from_zip $PATH_TO_MODEL_FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySyKkGA4b0R6"
      },
      "source": [
        "## The Cancer Imaging Archive (TCIA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhYD_mvRdKTd"
      },
      "source": [
        "This course assumes you have some basic familiarity with The Cancer Imaging Archive.  If you have never used TCIA you can [watch this presentation from RSNA 2020](https://vimeo.com/595989800) in order to understand the mission of TCIA and services it provides to the research community.  Options for accessing data from TCIA are summarized at https://www.cancerimagingarchive.net/access-data/. The two most relevant data access methods for this course are briefly summarized below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVyKi2oBfZxX"
      },
      "source": [
        "#### Browsing Collections & Analysis Results\n",
        "\n",
        "The most basic way to find data on TCIA is to [Browse Collections](https://www.cancerimagingarchive.net/collections) and [Browse Analysis Results](https://www.cancerimagingarchive.net/tcia-analysis-results/). Using the information in the table you can identify potential datasets of interest. Clicking on a given dataset takes you to a page which provides a description, data usage policy and citation guidelines, and links to download the data.  \n",
        "\n",
        "TCIA hosts a variety of image types and other related files, but the majority of its data are radiology images stored in DICOM format. When downloading DICOM images the download link will save a *.TCIA \"manifest\" file rather than the actual images. These manifest files must be opened with a helper application called the [NBIA Data Retriever](https://wiki.cancerimagingarchive.net/x/egOnAg). The Data Retriever can be installed on Windows, Mac and Linux operating systems. The Linux version also supports a command-line interface option which can be used on Google Colab.  \n",
        "\n",
        "#### Example\n",
        "Let's assume you are interested in lung cancer datasets which have both CT images and segmentations in DICOM format.  From the [Browse Collections](https://www.cancerimagingarchive.net/collections) page you can use the filter box (top right of the table of datasets) to filter out datasets of interest.  Try typing \"lung cancer CT\".  This should reduce the table to 24 results.  In order to find out which datasets also have segmentations you can add \"SEG\" or \"RTSTRUCT\" to the filter.  For the sake of this example, let's try using \"lung cancer CT seg\".  This should reduce the results to 3 datasets.  Let's assume that you find the [NSCLC-Radiomics-Interobserver1](https://doi.org/10.7937/tcia.2019.cwvlpd26) collection to be the most interesting.  Clicking on the link to this dataset in the table will open its summary page. \n",
        "\n",
        "After reviewing the page to learn more about this dataset, scroll down to the bottom \"Data Access\" section.  Click the blue \"Download\" button for the Images and Segmentations to save the associated manifest file.  You can then upload this file to Colab, and open it using the NBIA Data Retriever by running the following code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsm1MeREmdYm"
      },
      "source": [
        "# install NBIA Data Retriever for downloading images \n",
        "# documentation available at https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images\n",
        "\n",
        "!mkdir /usr/share/desktop-directories/\n",
        "!wget -P /content/NBIA-Data-Retriever https://cbiit-download.nci.nih.gov/nbia/releases/ForTCIA/NBIADataRetriever_4.2/nbia-data-retriever-4.2.deb\n",
        "!dpkg -i /content/NBIA-Data-Retriever/nbia-data-retriever-4.2.deb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJqE_XZCrHjB"
      },
      "source": [
        "# NBIA Data Retriever's Linux CLI documentation is at: https://wiki.cancerimagingarchive.net/display/NBIA/NBIA+Data+Retriever+Command+Line+Interface \n",
        "\n",
        "# TODO: is there a way to wget the file directly from the wiki to avoid manual steps of uploading the file into Colab?\n",
        "!wget -O manifest_nsclc.tcia https://wiki.cancerimagingarchive.net/download/attachments/52756590/NSCLC-RADIOMICS-INTEROBSERVER1-Aug%2031%202020-NBIA-manifest.tcia?version=1&modificationDate=1598890227618&api=v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdJhtWIAcOj5"
      },
      "source": [
        "## Imaging Data Commons (IDC)\n",
        "\n",
        "The National Cancer Institute (NCI) Cancer Research DataCommons (CRDC) [link text](https://datacommons.cancer.gov/) aims to establish a national cloud-based datascience infrastructure. Imaging Data Commons (IDC) is a newcomponent of CRDC supported by the Cancer Moonshot. The goal of IDC is to enable a broad spectrum of cancer researchers, with and without imaging expertise, to easily access and explore the value of deidentified imaging data and to support integrated analyses with nonimaging data. We achieve this goal by colocating versatile imaging collections with cloud-based computing resources and data exploration, visualization, and analysis tools. \n",
        "\n",
        "IDC provides access to curated imaging collections, accompanied by documentation, a user forum, and a growing number of analysis use cases that aim todemonstrate the value of a data commons framework applied to cancer imaging research.\n",
        "\n",
        "Key resources maintained by IDC are the following:\n",
        "* [public cancer imaging data](https://imaging.datacommons.cancer.gov/collections/) stored in Google Storage buckets and [public metadata tables](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=idc_current&page=dataset) that contain all of the DICOM metadata for IDC-hosted images\n",
        "  * these are now available in [Google Public Dataset Program](https://console.cloud.google.com/marketplace/product/gcp-public-data-idc/nci-idc-data)\n",
        "* [radiology](https://viewer.imaging.datacommons.cancer.gov/viewer/1.3.6.1.4.1.14519.5.2.1.6279.6001.224985459390356936417021464571) and [pathology](https://viewer.imaging.datacommons.cancer.gov/slim/studies/1.3.6.1.4.1.5962.99.1.2463087261.2121647220.1625960757917.3.0/series/1.3.6.1.4.1.5962.99.1.2463087261.2121647220.1625960757917.2.0) zero-footprint viewers that can be used to visualize any of the data hosted by IDC in your browser\n",
        "* radiology and pathology use cases: reproducible analysis workflows that operate on IDC data, [available as Colab notebooks](https://github.com/ImagingDataCommons/IDC-Examples/tree/master/notebooks)\n",
        "* [user portal](https://imaging.datacommons.cancer.gov/) that can be used to explore the data available in IDC, visualize images and annotations, and build cohorts\n",
        "* [API](https://api.imaging.datacommons.cancer.gov/v1/swagger) that can be used for programmatic operations with IDC cohorts\n",
        "\n",
        "At the moment, most of the data you will find in IDC has been replicated from TCIA (exception to this are DICOM-converted digital pathology collections). In the future, IDC will host cancer imaging data from sources other than TCIA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRtmCas7dT6D"
      },
      "source": [
        "## Setup of the Colab VM\n",
        "\n",
        "\n",
        "\n",
        "In the following cells we will confirm you have a GPU before doing anything else, and will install and import all the Python dependencies. \n",
        "\n",
        "The main python packages we need to install are:\n",
        "* `nnunet` - which is the [codebase for the nn-UNet framework](https://github.com/MIC-DKFZ/nnUNet) we are going to be using for the segmentation step;\n",
        "* `pydicom`, a Python [package](https://github.com/pydicom/pydicom) that lets the use read, modify, and write DICOM data in an easy \"pythonic\" way - that we are going to use to distinguish different DICOM objects from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLvysANUArnm"
      },
      "source": [
        "### GPU checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf2j172mddvK"
      },
      "source": [
        "# check wether the Colab Instance was correctly initialized with a GPU instance\n",
        "gpu_list = !nvidia-smi --list-gpus\n",
        "\n",
        "has_gpu = False if \"failed\" in gpu_list[0] else True\n",
        "\n",
        "if not has_gpu:\n",
        "  print(\"Your Colab VM does not have a GPU - check \\\"Runtime > Change runtime type\\\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL6xCvo3eKQ0"
      },
      "source": [
        "# check which model of GPU the notebook is equipped with - a Tesla K80 or T4\n",
        "# T4 is the best performing on the two - and can about half the GPU processing time\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJhwBz2ABT_p"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "Here we will configure the Linux environment variables needed to run the nnU-Net pipeline. \n",
        "\n",
        "Three main variables are needed by default to run the nnU-Net segmentation pipelines:\n",
        "* `nnUNet_raw_data_base` is the path to the folder where the segmentation pipeline expects to find the data to process;\n",
        "* `nnUNet_preprocessed` is the path to the folder where the preprocessed data are saved;\n",
        "* `RESULTS_FOLDER` is the path to the folder storing by default the model weights and, in our case, for simplicity, the segmentation masks produced by the pipeline.\n",
        "\n",
        "We will use the additional variable `PATH_TO_MODEL_FILE` to point to the location where the pre-trained model weights for the chosen model will be stored (more on this later).\n",
        "\n",
        "Please notice that these variables need to be set using `os.environ[]` in Google Colab - as `!export` is not sufficient to guarantee the variables are kept from one cell to the other. For more in-depth information regarding what the nnU-Net framework uses these folders for, please visit [the dedicated nnU-Net documentation page](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjiHm26cBdKm"
      },
      "source": [
        "# set env variables for the bash process\n",
        "import os\n",
        "os.environ['nnUNet_raw_data_base'] = \"/content/tutorial/data/nnUNet_raw_data/\"\n",
        "os.environ['nnUNet_preprocessed'] = \"/content/tutorial/data/processed/\"\n",
        "\n",
        "os.environ[\"RESULTS_FOLDER\"] = \"/content/tutorial/output/\"\n",
        "os.environ[\"PATH_TO_MODEL_FILE\"] = \"/content/tutorial/models/Task055_SegTHOR.zip\"\n",
        "\n",
        "dicom_sorted_dir = \"/content/tutorial/data/dicom\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40E3HnI5A0SX"
      },
      "source": [
        "### Install command-line tools\n",
        "\n",
        "\n",
        "The only package we will need for this tutorial is [Plastimatch](https://plastimatch.org/index.html). Plastimatch is considered to be the swiss army knife of medical images processing: we will use it to convert DICOM (CT, RTSTRUCT) series to NRRD files - but it can be used for a multitude of other tasks, such as registration, resampling, cropping, and computing statistics to name a few. Plastimatch is also available as a 3DSlicer plug-in and can be used directly from the Slicer GUI.\n",
        "\n",
        "For the sake of clarity and simplicity, we will call Plastimatch from a very simple [Python wrapper](https://github.com/denbonte/pyplastimatch) written for the occasion (unfortunately, Plastimatch does not provide an official one) - more on this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZL6-ByHA7XY"
      },
      "source": [
        "%%capture\n",
        "!sudo apt update\n",
        "\n",
        "!sudo apt install plastimatch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMovHnKgBEfC"
      },
      "source": [
        "!echo $(plastimatch --version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFRWendbBH72"
      },
      "source": [
        "We are also going to install subversion, a tool that will allow us to clone GitHub repositories only partially (to save time and space)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBEJRe-2BKah"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!sudo apt install subversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI1_BixWBN1O"
      },
      "source": [
        "!echo $(svn --version | head -n 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ev-JlpCAuMs"
      },
      "source": [
        "### Install Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XMzq-8nRvKd"
      },
      "source": [
        "%%capture\n",
        "!pip install nnunet\n",
        "!pip install pydicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cURjj8rzAa2L"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import time\n",
        "import gdown\n",
        "\n",
        "import json\n",
        "import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pydicom\n",
        "import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "\n",
        "from medpy.metric.binary import dc as dice_coef\n",
        "from medpy.metric.binary import hd as hausdorff_distance\n",
        "from medpy.metric.binary import asd as avg_surf_distance\n",
        "\n",
        "from medpy.filter.binary import largest_connected_component\n",
        "\n",
        "# use the \"tensorflow_version\" magic to make sure TF 1.x is imported\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "print(\"Python version               : \", sys.version.split('\\n')[0])\n",
        "print(\"Numpy version                : \", np.__version__)\n",
        "print(\"TensorFlow version           : \", tf.__version__)\n",
        "print(\"Keras (stand-alone) version  : \", keras.__version__)\n",
        "\n",
        "print(\"\\nThis Colab instance is equipped with a GPU.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "#everything that has to do with plotting goes here below\n",
        "import matplotlib\n",
        "matplotlib.use(\"agg\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"png\"\n",
        "\n",
        "import ipywidgets as ipyw\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# create new colormap appending the alpha channel to the selected one\n",
        "# (so that we don't get a \\\"color overlay\\\" when plotting the segmask superimposed to the CT)\n",
        "cmap = plt.cm.Reds\n",
        "my_reds = cmap(np.arange(cmap.N))\n",
        "my_reds[:,-1] = np.linspace(0, 1, cmap.N)\n",
        "my_reds = ListedColormap(my_reds)\n",
        "\n",
        "cmap = plt.cm.Greens\n",
        "my_greens = cmap(np.arange(cmap.N))\n",
        "my_greens[:,-1] = np.linspace(0, 1, cmap.N)\n",
        "my_greens = ListedColormap(my_greens)\n",
        "\n",
        "cmap = plt.cm.Blues\n",
        "my_blues = cmap(np.arange(cmap.N))\n",
        "my_blues[:,-1] = np.linspace(0, 1, cmap.N)\n",
        "my_blues = ListedColormap(my_blues)\n",
        "\n",
        "cmap = plt.cm.spring\n",
        "my_spring = cmap(np.arange(cmap.N))\n",
        "my_spring[:,-1] = np.linspace(0, 1, cmap.N)\n",
        "my_spring = ListedColormap(my_spring)\n",
        "## ----------------------------------------\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrAh2kjqb2Be"
      },
      "source": [
        "# PyPlastimatch - python wrapper for Plastimatch (and interactive notebook visualisation)\n",
        "!svn checkout https://github.com/AIM-Harvard/pyplastimatch/trunk/pyplastimatch tutorial/pyplastimatch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsePTNBmM9sw"
      },
      "source": [
        "# dicomsort is the pythong package that can sort DICOM files into\n",
        "# folder organization based on user-specified DICOM attributes\n",
        "!git clone https://github.com/pieper/dicomsort.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duil1tviGWaG"
      },
      "source": [
        "## Data selection\n",
        "\n",
        "The model is trained to segment organs in chest CT. In the following cell we initialize variable that points to the specific CT image (more precisely, specific DICOM CT image series) using DICOM `SeriesInstanceUID` attribute that we will use with the segmentation tool. Utilizing this unique identifier you can retrieve the imaging series from either TCIA or IDC.\n",
        "\n",
        "Once you are done with this example, we will give you instructions how to find more chest CT image series that you can use to experiment with the nnU-Net segmentation tool!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOBcXkDoHph1"
      },
      "source": [
        "my_SeriesInstanceUID = \"1.3.6.1.4.1.32722.99.99.232988001551799080335895423941323261228\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azGjFr-qG0us"
      },
      "source": [
        "## Visualization and download of data from IDC\n",
        "\n",
        "In order to work with Google Cloud, you will need to have a GCP project configured with billing enabled. If you completed the prerequisites, you should have project ID handy - please put into the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dhXZF07n8Le"
      },
      "source": [
        "# initialize this variable with your project ID!\n",
        "# if you are working with this notebook during RSNA 2021, completed the prerequisites\n",
        "# and your application has been approved, you should be able to use the project ID\n",
        "# specified here without changes\n",
        "my_ProjectID = \"idc-tcia\"\n",
        "\n",
        "import os\n",
        "os.environ[\"GCP_PROJECT_ID\"] = my_ProjectID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY1xyyuWVFE_"
      },
      "source": [
        "In the next cell your google account will be authenticated so that you can interact with the GCP resources. Follow the prompts and enter the verification code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDHCwK-TG4n2"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9YKMGVvVZX_"
      },
      "source": [
        "Given `SeriesInstanceUID` value identifying the image series, we can query the IDC metadata table to get the list of files (defined by the Google Storage URLs) corresponding to this series. You can perform this query using command line tools from Google Cloud SDK, BigQuery Python API, or with the `%%bigquery` magic. We will use Python API, since it is the easiest to parameterize.\n",
        "\n",
        "All of the DICOM metadata for each of the DICOM files is available in the BigQuery table we will be querying. We will get not just the `gcs_url`, but also identifiers for the Study, Series and Instance, to better understand organization of data, and since `StudyInstanceUID` will be handy later when we get to the visualization of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joRbZ81GWNz1"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "bq_client = bigquery.Client(my_ProjectID)\n",
        "\n",
        "selection_query = f\"\\\n",
        "  SELECT  \\\n",
        "    StudyInstanceUID, \\\n",
        "    SeriesInstanceUID, \\\n",
        "    SOPInstanceUID, \\\n",
        "    gcs_url \\\n",
        "  FROM \\\n",
        "    `bigquery-public-data.idc_current.dicom_all` \\\n",
        "  WHERE \\\n",
        "    SeriesInstanceUID = \\\"{my_SeriesInstanceUID}\\\"\"\n",
        "\n",
        "selection_result = bq_client.query(selection_query)\n",
        "selection_df = selection_result.result().to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgY51LXoYWzO"
      },
      "source": [
        "Let's look at the resulting table. Each row corresponds to a single DICOM file, which can be downloaded using the `gcs_url` URL. The values of `StudyInstanceUID` and `SeriesInstanceUID` are identical for all files, since they belong to the same study and series, but `SOPInstanceUID` values uniquely identify the specific DICOM file (instance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWf6sYpOYZ4S"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "selection_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFgSARVk9bUA"
      },
      "source": [
        "We will initiallize `StudyInstanceUID` as we will need it for several steps in the following cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UALQQLrT9mTq"
      },
      "source": [
        "import pandas as pd\n",
        "my_StudyInstanceUIDs = selection_df['StudyInstanceUID'].unique()\n",
        "my_StudyInstanceUIDs.sort()\n",
        "my_StudyInstanceUID = my_StudyInstanceUIDs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG2USrvUd7yl"
      },
      "source": [
        "We can use the IDC radiology image viewer to visualize the series and study of interest. This can be done using the identifiers we have in the dataframe we obtained in the earlier query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HawHLzymeGcb"
      },
      "source": [
        "def get_idc_viewer_url(studyUID, seriesUID=None):\n",
        "  url = \"https://viewer.imaging.datacommons.cancer.gov/viewer/\"+studyUID\n",
        "  if seriesUID is not None:\n",
        "    url = url+\"?seriesInstanceUID=\"+seriesUID\n",
        "  return url\n",
        "\n",
        "print(\"URL to view the entire study:\")\n",
        "print(get_idc_viewer_url(my_StudyInstanceUID))\n",
        "print()\n",
        "print(\"URL to view the specific series:\")\n",
        "print(get_idc_viewer_url(my_StudyInstanceUID, my_SeriesInstanceUID))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRfoM4AXcmCX"
      },
      "source": [
        "Now that we confirmed the series we identified is indeed a CT of the chest, let's download the DICOM files for this series to the Colab VM. We will use the `gsutil` command line tool to fetch each of the files defined by the list of GCS URLs saved in a plain text file.\n",
        "\n",
        "If you want to download large number of files from IDC, make sure to check out [this documentation article](https://learn.canceridc.dev/data/downloading-data) to learn about performance optimizations of the download!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4YXrk6PIkJe"
      },
      "source": [
        "# save the list of GCS URLs into a file\n",
        "import os\n",
        "idc_download_folder = \"/content/IDC_downloads\"\n",
        "if not os.path.exists(idc_download_folder):\n",
        "  os.mkdir(idc_download_folder)\n",
        "\n",
        "selection_manifest = os.path.join(idc_download_folder, \"idc_manifest.txt\")\n",
        "selection_df[\"gcs_url\"].to_csv(selection_manifest, header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIKSKvW8mdsN"
      },
      "source": [
        "# confirm the resulting manifest has as many lines as the number of rows in the\n",
        "# dataframe we initialized earlier\n",
        "!cat /content/IDC_downloads/idc_manifest.txt |wc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdZSVftdjFPl"
      },
      "source": [
        "# let's make sure the download folder is clean, in case you ran this cell earlier\n",
        "# for a different dataset\n",
        "!rm -rf /content/IDC_downloads/*.dcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ydwu1tSYScJ"
      },
      "source": [
        "# download is this simple!\n",
        "%%capture\n",
        "\n",
        "!cat /content/IDC_downloads/idc_manifest.txt | gsutil -m cp -I /content/IDC_downloads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WqUQnULEbPN"
      },
      "source": [
        "\n",
        "Now you have the data ready for the next steps of the processing. If you are interested how to download the same series using TCIA API, continue to the next section. Otherwise you can skip to **Sort the DICOM files**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxJzr-GDIlAV"
      },
      "source": [
        "## Download of data from TCIA\n",
        "\n",
        "For the sake of simplicity, we will download images for a specific case that we know has CT of the chest, and segmentations of the organs of interest. This time, let's utilize the [NBIA REST API](https://wiki.cancerimagingarchive.net/x/fILTB) instead of the NBIA Data Retriever to download the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfWQjFEQNEbJ"
      },
      "source": [
        "import requests\n",
        "params = {\"Collection\":my_CollectionID, \"PatientID\":my_PatientID}\n",
        "r = requests.get(\"https://services.cancerimagingarchive.net/nbia-api/services/v1/getSeries\", params=params)\n",
        "if r.status_code == 200:\n",
        "  df = pd.read_json(r.text)\n",
        "else:\n",
        "  print(f\"Failed with {r.status_code}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBdRxH9emEsw"
      },
      "source": [
        "import json\n",
        "tcia_manifest_json = json.loads(r.text)\n",
        "\n",
        "\n",
        "tcia_selection_df = pd.read_json(r.text)\n",
        "\n",
        "distinct_StudyInstanceUIDs = selection_df['StudyInstanceUID'].unique()\n",
        "distinct_StudyInstanceUIDs.sort()\n",
        "print(\"Distinct values of StudyInstanceUID:\")\n",
        "print('\\n'.join(distinct_StudyInstanceUIDs))\n",
        "\n",
        "study_uid = distinct_StudyInstanceUIDs[0]\n",
        "print(f\"\\nStudy that will be analyzed: {study_uid}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOYNkKApoNFr"
      },
      "source": [
        "!mkdir -p /content/TCIA_downloads && rm -rf /content/TCIA_downloads/*.dcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fASAVcz2GG3j"
      },
      "source": [
        "# download zip file with the series instances\n",
        "import requests, os, zipfile\n",
        "\n",
        "tcia_download_folder = \"/content/TCIA_downloads\"\n",
        "params = {\"SeriesInstanceUID\":my_SeriesInstanceUID}\n",
        "image_request = requests.get(\" https://services.cancerimagingarchive.net/nbia-api/services/v1/getImage\", params=params, stream=True)\n",
        "print(f\"Completed request: {image_request.url}\")\n",
        "if image_request.status_code == 200:\n",
        "  series_zip_name = os.path.join(tcia_download_folder, f\"{my_SeriesInstanceUID}.zip\")\n",
        "  with open(series_zip_name, \"wb\") as f:\n",
        "    for chunk in image_request.iter_content(chunk_size=1024):\n",
        "      f.write(chunk)\n",
        "\n",
        "  # extract individual instances from the series zip file\n",
        "  series_folder_name = os.path.join(tcia_download_folder, my_SeriesInstanceUID)\n",
        "  if not os.path.exists(series_folder_name):\n",
        "    os.mkdir(series_folder_name)\n",
        "  with zipfile.ZipFile(series_zip_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(series_folder_name)\n",
        "else:\n",
        "  print(f\"Failed with {r.status_code}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skFm7Y-6Mu9L"
      },
      "source": [
        "import json, zipfile\n",
        "\n",
        "series_list = tcia_selection_df[tcia_selection_df[\"StudyInstanceUID\"]==study_uid][\"SeriesInstanceUID\"].unique()\n",
        "\n",
        "tcia_download_folder = \"/content/TCIA_downloads\"\n",
        "if not os.path.exists(tcia_download_folder):\n",
        "  os.mkdir(tcia_download_folder)\n",
        "\n",
        "for series_uid in series_list:\n",
        "\n",
        "  # download zip file with the series instances\n",
        "  params = {\"SeriesInstanceUID\":series_uid}\n",
        "  image_request = requests.get(\" https://services.cancerimagingarchive.net/nbia-api/services/v1/getImage\", params=params, stream=True)\n",
        "  print(f\"Completed request: {image_request.url}\")\n",
        "  if image_request.status_code == 200:\n",
        "    series_zip_name = os.path.join(tcia_download_folder, f\"{series_uid}.zip\")\n",
        "    with open(series_zip_name, \"wb\") as f:\n",
        "      for chunk in image_request.iter_content(chunk_size=1024):\n",
        "        f.write(chunk)\n",
        "    print(f\"Downloaded and saved series {series_uid}\")\n",
        "\n",
        "    # extract individual instances from the series zip file\n",
        "    series_folder_name = os.path.join(tcia_download_folder, series_uid)\n",
        "    if not os.path.exists(series_folder_name):\n",
        "      os.mkdir(series_folder_name)\n",
        "    with zipfile.ZipFile(series_zip_name, 'r') as zip_ref:\n",
        "      zip_ref.extractall(series_folder_name)\n",
        "  else:\n",
        "    print(f\"Failed with {r.status_code}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JzSPxXD6AYd"
      },
      "source": [
        "Now we will extract DICOM attributes of interset to enable more convenient exploration and subsetting of series within the study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cip5y5A6HeZ"
      },
      "source": [
        "import glob\n",
        "import pydicom\n",
        "\n",
        "selection_dict = []\n",
        "for root, _, files in os.walk(tcia_download_folder):\n",
        "  for file in files:\n",
        "    if file.endswith(\".dcm\"):\n",
        "      dcm = pydicom.read_file(os.path.join(root, file), stop_before_pixels=True)\n",
        "      dict_item = {}\n",
        "      for attr in dicom_attributes:\n",
        "        try:\n",
        "          dict_item[attr] = dcm.data_element(attr).value\n",
        "        except (AttributeError, KeyError) as e:\n",
        "          #print(f\"Failed to find {attr} in {file}! Skipping.\")\n",
        "          dict_item[attr] = None\n",
        "      selection_dict.append(dict_item)\n",
        "tcia_selection_df = pd.DataFrame(selection_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAuPIshF91Tj"
      },
      "source": [
        "tcia_selection_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oER5RpuIUYi"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "selection_df[selection_df[\"StudyInstanceUID\"]==target_StudyInstanceUID].groupby(['SeriesInstanceUID','Modality']).size().reset_index().rename(columns={0:'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yrVZazpHvsX"
      },
      "source": [
        "selection_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOAQXiqNhVqR"
      },
      "source": [
        "### %%bigquery --project=idc-tcia  --params={\"patient_id\":\"LUNG1-002\",\"collection_id\":\"NSCLC-Radiomics\"} case002_df \n",
        "dicom_attributes = [\"PatientID\", \"StudyInstanceUID\", \"SeriesInstanceUID\", \"SOPInstanceUID\", \"Modality\", \"SeriesDescription\"]\n",
        "dicom_attributes_str = ','.join(dicom_attributes)\n",
        "dicom_attributes_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEWsk41Mnw35"
      },
      "source": [
        "TODO: replace the below with BQ python client to simplify parameterization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ZNHX_2Gw8O"
      },
      "source": [
        "%%bigquery --project=idc-tcia  --params={\"patient_id\":\"100014\",\"collection_id\":\"NLST\"} selection_df \n",
        "\n",
        "WITH\n",
        "  idc_manifest AS (\n",
        "  SELECT\n",
        "    PatientID,\n",
        "    StudyInstanceUID,\n",
        "    SeriesInstanceUID,\n",
        "    SOPInstanceUID,\n",
        "    Modality,\n",
        "    SeriesDescription,\n",
        "    gcs_url,\n",
        "    collection_id as idc_collection_id\n",
        "  FROM\n",
        "    `bigquery-public-data.idc_current.dicom_all`\n",
        "  WHERE\n",
        "    PatientID = @patient_id\n",
        ")\n",
        "SELECT\n",
        "  idc_manifest.*,\n",
        "  # this is necessary since collection IDs used internally by IDC and TCIA are a bit different,\n",
        "  # so we need to get the TCIA collection ID that will be recognized by TCIA API\n",
        "  aux_table.tcia_api_collection_id\n",
        "FROM\n",
        "  idc_manifest\n",
        "JOIN\n",
        "  `bigquery-public-data.idc_current.auxiliary_metadata` AS aux_table\n",
        "ON\n",
        "  idc_manifest.SOPInstanceUID = aux_table.SOPInstanceUID\n",
        "WHERE \n",
        "    # PatientID is unique and parameterization by collection_id is not really necessary,\n",
        "    # but we use it here for consistency with the query we use with NBIA API, which does\n",
        "    # require collection ID to be specified\n",
        "    aux_table.tcia_api_collection_id = @collection_id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zy1EmGtdCy5"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "bq_client = bigquery.Client(my_ProjectID)\n",
        "\n",
        "selection_query = f\"\\\n",
        "  WITH idc_manifest AS ( \\\n",
        "  SELECT {dicom_attributes_str}, \\\n",
        "    gcs_url, \\\n",
        "    collection_id as idc_collection_id \\\n",
        "  FROM \\\n",
        "    `bigquery-public-data.idc_current.dicom_all` \\\n",
        "  WHERE \\\n",
        "    PatientID = \\\"{my_PatientID}\\\" \\\n",
        ") \\\n",
        "SELECT \\\n",
        "  idc_manifest.*, \\\n",
        "  # this is necessary since collection IDs used internally by IDC and TCIA are a bit different,\\n \\\n",
        "  # so we need to get the TCIA collection ID that will be recognized by TCIA API\\n \\\n",
        "  aux_table.tcia_api_collection_id \\\n",
        "FROM \\\n",
        "  idc_manifest \\\n",
        "JOIN \\\n",
        "  `bigquery-public-data.idc_current.auxiliary_metadata` AS aux_table \\\n",
        "ON \\\n",
        "  idc_manifest.SOPInstanceUID = aux_table.SOPInstanceUID \\\n",
        "WHERE \\\n",
        "    # PatientID is unique and parameterization by collection_id is not really necessary,\\n \\\n",
        "    # but we use it here for consistency with the query we use with NBIA API, which does\\n \\\n",
        "    # require collection ID to be specified\\n \\\n",
        "    aux_table.tcia_api_collection_id = \\\"{my_CollectionID}\\\"\" \n",
        "\n",
        "print(selection_query)\n",
        "selection_result = bq_client.query(selection_query)\n",
        "selection_df = selection_result.result().to_dataframe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvlSBfPRdRpz"
      },
      "source": [
        "## Sort the DICOM files\n",
        "\n",
        "Let's check the content downloaded from the two repositories is identical, just in case. This section will apply only if you downloaded the data both from IDC and TCIA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aVI7qAbaT0"
      },
      "source": [
        "!mkdir -p IDC_sorted && mkdir -p TCIA_sorted\n",
        "!python dicomsort/dicomsort.py -k -u IDC_downloads IDC_sorted/%PatientID/%StudyInstanceUID/%SeriesInstanceUID/%SOPInstanceUID.dcm\n",
        "!python dicomsort/dicomsort.py -k -u TCIA_downloads TCIA_sorted/%PatientID/%StudyInstanceUID/%SeriesInstanceUID/%SOPInstanceUID.dcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgBtSr1yHEUc"
      },
      "source": [
        "Run the cell below only if you downloaded the image series using both IDC and TCIA routes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNDv9Eq5czJx"
      },
      "source": [
        "# compare if the files downloaded for the series are the same between IDC and TCIA\n",
        "!diff -r IDC_sorted TCIA_sorted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewcZsa1rzKHi"
      },
      "source": [
        "Move the sorted data into the right place"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ_LAvUGEGxj"
      },
      "source": [
        "!mkdir -p tutorial/data/dicom && rm -rf tutorial/data/dicom/* && mv IDC_sorted/* tutorial/data/dicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jerJXiNbcgo"
      },
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "In order to run the AI segmentation pipeline, we need to convert the DICOM data in a format required by nnU-Net.\n",
        "\n",
        "Using the simple Plastimatch wrapper, let's convert the DICOM CT series in both NRRD (very flexible, simple handling with SimpleITK) and NIfTI (as required by the nnU-Net pipeline) format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7JuZhZbiXP"
      },
      "source": [
        "from tutorial.pyplastimatch import pyplastimatch as pypla\n",
        "from tutorial.pyplastimatch.utils import viz as viz_utils\n",
        "from tutorial.pyplastimatch.utils import data as data_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gLUnOsJDmaU"
      },
      "source": [
        "pat = os.listdir(dicom_sorted_dir)[0]\n",
        "\n",
        "# study_uid was initialized earlier, when we decided which study to download\n",
        "\n",
        "# directories storing NRRD and NIfTI files\n",
        "base_preproc_path = \"tutorial/data/processed\"\n",
        "\n",
        "pat_dir_path_nrrd = os.path.join(base_preproc_path, \"nrrd\", pat)\n",
        "pat_dir_path_nii = os.path.join(base_preproc_path, \"nii\", pat)\n",
        "  \n",
        "# patient subfolder where all the preprocessed NRRDs will be stored\n",
        "if not os.path.exists(pat_dir_path_nrrd): os.makedirs(pat_dir_path_nrrd)\n",
        "  \n",
        "# patient subfolder where all the preprocessed NIfTIs will be stored\n",
        "if not os.path.exists(pat_dir_path_nii): os.makedirs(pat_dir_path_nii)\n",
        "\n",
        "# path to the directory where the DICOM CT file is stored\n",
        "path_to_ct_dir = os.path.join(\"tutorial/data/dicom\", pat,\n",
        "                              my_StudyInstanceUID, my_SeriesInstanceUID)\n",
        "\n",
        "# path to the files where the NRRD and NIfTI CTs will be stored\n",
        "ct_nrrd_path = os.path.join(pat_dir_path_nrrd, pat + \"_ct.nrrd\")\n",
        "ct_nii_path = os.path.join(pat_dir_path_nii, pat + \"_ct.nii.gz\")\n",
        "\n",
        "verbose = True\n",
        "\n",
        "# logfile for the plastimatch conversion\n",
        "log_file_path_nrrd = os.path.join(pat_dir_path_nrrd, pat + '_pypla.log')\n",
        "log_file_path_nii = os.path.join(pat_dir_path_nii, pat + '_pypla.log')\n",
        "  \n",
        "# DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "if not os.path.exists(ct_nrrd_path):\n",
        "  convert_args_ct = {\"input\" : path_to_ct_dir,\n",
        "                     \"output-img\" : ct_nrrd_path}\n",
        "  \n",
        "  # clean old log file if it exist\n",
        "  if os.path.exists(log_file_path_nrrd): os.remove(log_file_path_nrrd)\n",
        "  \n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_nrrd, **convert_args_ct)\n",
        "\n",
        "# DICOM CT to NIfTI conversion (if the file doesn't exist yet)\n",
        "if not os.path.exists(ct_nii_path):\n",
        "  convert_args_nii = {\"input\" : path_to_ct_dir, \n",
        "                      \"output-img\" : ct_nii_path}\n",
        "  \n",
        "  # clean old log file if it exist\n",
        "  if os.path.exists(log_file_path_nii): os.remove(log_file_path_nii)\n",
        "  \n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_nii, **convert_args_nii)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcYxd4NAFNps"
      },
      "source": [
        "As the final step before running the lung nodules segmentation pipeline, we need to make sure the folder storing the data follows the structure required by the nnU-Net framework, described at the [dedicated documentation page](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_conversion.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CFhNcscFOtA"
      },
      "source": [
        "# create a folder (random task name) for nnU-Net inference\n",
        "proc_folder_path = os.path.join(os.environ[\"nnUNet_raw_data_base\"],\n",
        "                                \"segthor\", \"imagesTs\")\n",
        "\n",
        "!mkdir -p $proc_folder_path\n",
        "\n",
        "# populate the folder following the nnU-Net naming conventions\n",
        "copy_path = os.path.join(proc_folder_path, pat + \"_0000.nii.gz\")\n",
        "\n",
        "# copy NIfTI to the right dir for nnU-Net processing\n",
        "if not os.path.exists(copy_path):\n",
        "  shutil.copy(src = ct_nii_path, dst = copy_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ci1vFuJTlG"
      },
      "source": [
        "## Segmentation of thoracic structures from CT series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvazHm6AFXVx"
      },
      "source": [
        "### Inference \n",
        "\n",
        "In order to run the segmentation pipeline, we can follow the [\"run inference\" section of the nnU-Net documentation](https://github.com/MIC-DKFZ/nnUNet#how-to-run-inference-with-pretrained-models), specifying the path to the input and output folders defined in the sections above, and the pretrained model we want to use (i.e., the one we downloaded earlier).\n",
        "\n",
        "For the purpose of this notebook, to make the processing faster, we are not going to use an ensemble of different U-Net configurations for inference or test time augmentation (TTA). You are invited to explore these options later - and if you decide to do so, you can read [this example](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/inference_example_Prostate.md) from the nnU-Net documentation to learn how this can be achieved.\n",
        "\n",
        "To learn more about all the arguments that can be specified to the `nnUNet_predict` command, run `nnUNet_predict --help`.\n",
        "\n",
        "The following step will take a couple of minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAEBKIbQFgLw"
      },
      "source": [
        "# run the inference phase\n",
        "# accepted options for --model are: 2d, 3d_lowres, 3d_fullres or 3d_cascade_fullres\n",
        "!nnUNet_predict --input_folder \"tutorial/data/nnUNet_raw_data/segthor/imagesTs\" \\\n",
        "                --output_folder $RESULTS_FOLDER \\\n",
        "                --task_name \"Task055_SegTHOR\" --model 2d --disable_tta "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu7UIDM6JY2Z"
      },
      "source": [
        "### Post-processing of inference results\n",
        "\n",
        "After the inference is finished, we can convert the segmentation masks back to NRRD for visualisation purposes and for easier handling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq1fP6XHJfoS"
      },
      "source": [
        "pred_nii_path = os.path.join(os.environ[\"RESULTS_FOLDER\"], pat + \".nii.gz\")\n",
        "\n",
        "sitk_ct = sitk.ReadImage(ct_nrrd_path)\n",
        "\n",
        "nrrd_spacing = sitk_ct.GetSpacing()\n",
        "nrrd_dim = sitk_ct.GetSize()\n",
        "\n",
        "nii_spacing = tuple(nib.load(pred_nii_path).header['pixdim'][1:4])\n",
        "nii_dim = tuple(nib.load(pred_nii_path).get_fdata().shape)\n",
        "\n",
        "assert (nrrd_spacing == nii_spacing) & (nrrd_dim == nii_dim)\n",
        "\n",
        "## ----------------------------------------\n",
        "# NIfTI TO NRRD CONVERSION\n",
        "\n",
        "# path to the output NRRD file (inferred segmasks)\n",
        "pred_nrrd_path = os.path.join(pat_dir_path_nrrd, pat + \"_pred_segthor.nrrd\")\n",
        "log_file_path = os.path.join(pat_dir_path_nrrd, pat + \"_pypla.log\")\n",
        "\n",
        "# Inferred NIfTI segmask to NRRD\n",
        "convert_args_pred = {\"input\" : pred_nii_path, \n",
        "                     \"output-img\" : pred_nrrd_path}\n",
        "\n",
        "pypla.convert(path_to_log_file = log_file_path, **convert_args_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShIjT6jEJkK-"
      },
      "source": [
        "### Visualising segmentation results\n",
        "\n",
        "We can visualise the raw AI-inferred segmentation mask (heart, aorta, esophagus, amd treachea - in green, yellow, red, and blue, respectively) and compare the heart (and esophagus, if available for the randomly selected patient) segmentation to the manual delineation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p__wFByuJjrL"
      },
      "source": [
        "# load NRRD volumes\n",
        "ct_nrrd = sitk.GetArrayFromImage(sitk_ct)\n",
        "\n",
        "# inferred segmask\n",
        "pred_nrrd_segthor = sitk.GetArrayFromImage(sitk.ReadImage(pred_nrrd_path))\n",
        "\n",
        "pred_nrrd_esophagus = np.copy(pred_nrrd_segthor)\n",
        "pred_nrrd_heart = np.copy(pred_nrrd_segthor)\n",
        "pred_nrrd_trachea = np.copy(pred_nrrd_segthor)\n",
        "pred_nrrd_aorta = np.copy(pred_nrrd_segthor)\n",
        "  \n",
        "# zero every segmask other than the esophagus and make the mask binary (0/1)\n",
        "pred_nrrd_esophagus[pred_nrrd_segthor != 1] = 0\n",
        "pred_nrrd_esophagus[pred_nrrd_esophagus != 0] = 1\n",
        "  \n",
        "# zero every segmask other than the heart and make the mask binary (0/1)\n",
        "pred_nrrd_heart[pred_nrrd_segthor != 2] = 0\n",
        "pred_nrrd_heart[pred_nrrd_heart != 0] = 1\n",
        "  \n",
        "# zero every segmask other than the trachea and make the mask binary (0/1)\n",
        "pred_nrrd_trachea[pred_nrrd_segthor != 3] = 0\n",
        "pred_nrrd_trachea[pred_nrrd_trachea != 0] = 1\n",
        "  \n",
        "# zero every segmask other than the aorta and make the mask binary (0/1)\n",
        "pred_nrrd_aorta[pred_nrrd_segthor != 4] = 0\n",
        "pred_nrrd_aorta[pred_nrrd_aorta != 0] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TiIfDIvQ317"
      },
      "source": [
        "_ = viz_utils.AxialSliceSegmaskViz(ct_volume = ct_nrrd,\n",
        "                                          segmask_dict = {\"Heart\" : pred_nrrd_heart,\n",
        "                                                             \"Aorta\" : pred_nrrd_aorta,\n",
        "                                                             \"Trachea\" : pred_nrrd_trachea,\n",
        "                                                             \"Esophagus\" : pred_nrrd_esophagus},\n",
        "                                          segmask_cmap_dict = {\"Heart\" : my_greens,\n",
        "                                                               \"Aorta\" : my_spring,\n",
        "                                                               \"Esophagus\" : my_reds,\n",
        "                                                               \"Trachea\" : my_blues},\n",
        "                                          dpi = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063LqMlMiRdl"
      },
      "source": [
        "## Comparison of segmentations with existing annotations\n",
        "\n",
        "**NOTE: The cells in this section are applicable for the specific study/series we use as the example (`SeriesInstanceUID = 1.3.6.1.4.1.32722.99.99.203715003805996641695765332389135385095`). If you experiment with other series, this will either not be applicable, or you will need to adjust the code.**\n",
        "\n",
        "Many of the collections contain annotations alongside images, which can be used to train new algorithms, or to evaluate performance of algorithms. We can view the entire imaging study, as we did earlier, to see what annotations are available for the series we segmented. As you can see, this study contains segmentations of organs saved in RTSTRUCT and SEG series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvu-rxiQnDxp"
      },
      "source": [
        "print(get_idc_viewer_url(my_StudyInstanceUID))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXiHlbunkYh"
      },
      "source": [
        "We can get the names of the structures in the RTSTRUCT series that are available in the same DICOM study using the following query:\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  SeriesInstanceUID,\n",
        "  structureSetROISequence.ROIName AS ROIName\n",
        "FROM\n",
        " `bigquery-public-data.idc_current.dicom_all`\n",
        "CROSS JOIN\n",
        " UNNEST (StructureSetROISequence) AS structureSetROISequence\n",
        "WHERE\n",
        " Modality = \"RTSTRUCT\" AND StudyInstanceUID = <my_StudyInstanceUID>\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPSF-l5siWKk"
      },
      "source": [
        "rt_selection_query = f\"\\\n",
        "  SELECT \\\n",
        "    SeriesInstanceUID,\\\n",
        "    structureSetROISequence.ROIName AS ROIName, \\\n",
        "    gcs_url \\\n",
        "  FROM \\\n",
        "    `bigquery-public-data.idc_current.dicom_all` \\\n",
        "  CROSS JOIN \\\n",
        "    UNNEST (StructureSetROISequence) AS structureSetROISequence \\\n",
        "  WHERE \\\n",
        "    Modality = \\\"RTSTRUCT\\\" AND StudyInstanceUID = \\\"{my_StudyInstanceUID}\\\"\"\n",
        "\n",
        "rt_selection_result = bq_client.query(rt_selection_query)\n",
        "rt_selection_df = rt_selection_result.result().to_dataframe()\n",
        "\n",
        "rt_selection_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYG-4UAXq4Eb"
      },
      "source": [
        "As you can see, in this specific case, there is a single RTSTRUCT series that has segmentations of the structures that we also segmented in the previous step using nnU-Net. Let's download the corresponding DICOM file, and convert RTSTRUCT into a representation that we can use for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NhspiuVspVD"
      },
      "source": [
        "!mkdir -p /contents/IDC_downloads/RTSTRUCT\n",
        "!rm -rf /content/IDC_downloads/RTSTRUCT/*.dcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBAlYZY5scjR"
      },
      "source": [
        "path_to_rt_dir = \"/contents/IDC_downloads/RTSTRUCT\"\n",
        "rt_selection_manifest = \"/content/IDC_downloads/rt_manifest.txt\"\n",
        "rt_selection_df[\"gcs_url\"].to_csv(rt_selection_manifest, header=False, index=False)\n",
        "\n",
        "!cat /content/IDC_downloads/rt_manifest.txt | gsutil -m cp -I /contents/IDC_downloads/RTSTRUCT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da_0aVnyiW3C"
      },
      "source": [
        "# path to the files where the NRRD RTSTRUCTs will be stored\n",
        "rt_folder = os.path.join(pat_dir_path_nrrd, \"RTSTRUCT\")\n",
        "rt_list_path = os.path.join(pat_dir_path_nrrd, \"RTSTRUCT_content\")\n",
        "\n",
        "# DICOM RTSTRUCT to NRRD conversion (if the file doesn't exist yet)\n",
        "if not os.path.exists(rt_folder):\n",
        "  convert_args_rt = {\"input\" : path_to_rt_dir, \n",
        "                     \"referenced-ct\" : path_to_ct_dir,\n",
        "                     \"output-prefix\" : rt_folder,\n",
        "                     \"prefix-format\" : 'nrrd',\n",
        "                     \"output-ss-list\" : rt_list_path}\n",
        "  \n",
        "  # clean old log file if it exist\n",
        "  if os.path.exists(log_file_path_nrrd): os.remove(log_file_path_nrrd)\n",
        "  \n",
        "  pypla.convert(verbose = verbose, path_to_log_file = log_file_path_nrrd, **convert_args_rt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02Kn4GcmiVro"
      },
      "source": [
        "# manual segmask (from the RTSTRUCT)\n",
        "rt_segmask_heart = os.path.join(pat_dir_path_nrrd, \"RTSTRUCT\", \"Heart.nrrd\")\n",
        "rt_nrrd_heart = sitk.GetArrayFromImage(sitk.ReadImage(rt_segmask_heart))\n",
        "\n",
        "try:\n",
        "  rt_segmask_esophagus = os.path.join(pat_dir_path_nrrd, \"RTSTRUCT\", \"Esophagus.nrrd\")\n",
        "  rt_nrrd_esophagus = sitk.GetArrayFromImage(sitk.ReadImage(rt_segmask_esophagus))\n",
        "except:\n",
        "  # for the sake of simplicity, fill the volume with zeros\n",
        "  # (so that we can keep the code that comes after the same)\n",
        "  rt_nrrd_esophagus = np.zeros(rt_nrrd_heart.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Ll9UABJsFg"
      },
      "source": [
        "_ = viz_utils.AxialSliceSegmaskComparison(ct_volume = ct_nrrd,\n",
        "                                          segmask_ai_dict = {\"Heart\" : pred_nrrd_heart,\n",
        "                                                             \"Aorta\" : pred_nrrd_aorta,\n",
        "                                                             \"Trachea\" : pred_nrrd_trachea,\n",
        "                                                             \"Esophagus\" : pred_nrrd_esophagus},\n",
        "                                          segmask_manual_dict = {\"Heart\" : rt_nrrd_heart,\n",
        "                                                                 \"Esophagus\" : rt_nrrd_esophagus},\n",
        "                                          segmask_cmap_dict = {\"Heart\" : my_greens,\n",
        "                                                               \"Aorta\" : my_spring,\n",
        "                                                               \"Esophagus\" : my_reds,\n",
        "                                                               \"Trachea\" : my_blues},\n",
        "                                          dpi = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhfNqPxNJ-xz"
      },
      "source": [
        "### Quantitative assessment of the results\n",
        "\n",
        "Let's start by defining a function to compute the center of mass (CoM) of the segmentation masks. Before computing the common segmentation metrics, the CoM can give us a rough idea of how different the 3D delineations are and if there are any major labelling errors (which we could correct, e.g., with a largest connected component analysis).\n",
        "\n",
        "We will base our function on the [implementation](https://github.com/AIM-Harvard/pyradiomics/blob/master/radiomics/generalinfo.py) found in the open source [PyRadiomics library](https://github.com/AIM-Harvard/pyradiomics)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYF05ymwKMhE"
      },
      "source": [
        "def getCenterOfMassIndexValue(input_mask):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns z, y and x coordinates of the center of mass of the ROI in terms of\n",
        "    the image coordinate space (continuous index).\n",
        "\n",
        "    Calculation is based on the original (non-resampled) mask.\n",
        "    Because this represents the continuous index, the order of x, y and z is reversed,\n",
        "    i.e. the first element is the z index, the second the y index and the last element is the x index.\n",
        "\n",
        "    @params:\n",
        "      input_mask - required : numpy (binary) volume storing the segmentation mask.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if input_mask is not None:\n",
        "      mask_coordinates = np.array(np.where(input_mask == 1))\n",
        "      center_index = np.mean(mask_coordinates, axis = 1)\n",
        "      return tuple(center_index)\n",
        "    else:\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl_qnwbuKOn8"
      },
      "source": [
        "com_manual_heart = np.array(getCenterOfMassIndexValue(rt_nrrd_heart))\n",
        "com_manual_heart_int = np.ceil(com_manual_heart).astype(dtype = np.uint16)\n",
        "\n",
        "com_raw_heart = np.array(getCenterOfMassIndexValue(pred_nrrd_heart))\n",
        "com_raw_heart_int = np.ceil(com_raw_heart).astype(dtype = np.uint16)\n",
        "\n",
        "print(\"Heart Center of Mass (raw AI segmentation) \\t:\", com_raw_heart_int)\n",
        "print(\"Heart Center of Mass (manual segmentation) \\t:\", com_manual_heart_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnN020q0KS_e"
      },
      "source": [
        "com_manual_heart = np.array(getCenterOfMassIndexValue(rt_nrrd_heart))\n",
        "com_manual_heart_int = np.ceil(com_manual_heart).astype(dtype = np.uint16)\n",
        "\n",
        "com_raw_heart = np.array(getCenterOfMassIndexValue(pred_nrrd_heart))\n",
        "com_raw_heart_int = np.ceil(com_raw_heart).astype(dtype = np.uint16)\n",
        "\n",
        "print(\"Heart Center of Mass (raw AI segmentation) \\t:\", com_raw_heart_int)\n",
        "print(\"Heart Center of Mass (manual segmentation) \\t:\", com_manual_heart_int)\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# run this if and only if a manual esophagus segmentation mask is available\n",
        "if np.sum(rt_nrrd_esophagus):\n",
        "\n",
        "  com_manual_esophagus = np.array(getCenterOfMassIndexValue(rt_nrrd_esophagus))\n",
        "  com_manual_esophagus_int = np.ceil(com_manual_esophagus).astype(dtype = np.uint16)\n",
        "\n",
        "  com_raw_esophagus = np.array(getCenterOfMassIndexValue(pred_nrrd_esophagus))\n",
        "  com_raw_esophagus_int = np.ceil(com_raw_esophagus).astype(dtype = np.uint16)\n",
        "\n",
        "  print(\"\\nEsophagus Center of Mass (raw AI segmentation) \\t:\", com_raw_esophagus_int)\n",
        "  print(\"Esophagus Center of Mass (manual segmentation) \\t:\", com_manual_esophagus_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYBAo8SzKW31"
      },
      "source": [
        "Another common way to evaluate the quality of the segmentation is computing the Dice Coefficient between the AI segmentation and the manual one. To do so, we will use [MedPy's implementation of the Dice coefficient](https://loli.github.io/medpy/generated/medpy.metric.binary.dc.html#medpy-metric-binary-dc) (for binary masks).\n",
        "\n",
        "We can use other MedPy's functions to compute the Hausdorff distance and the average surface distance as well*.\n",
        "\n",
        "_*in most cases, the Hausdorff Distance will be quite high for both the heart segmentation and, if available with the randomly selected patient, the esophagus one. This is not a clear indication the model performance is poor: rather, it could also be the segmentation guidelines of the two datasets (the one the nnU-Net model was trained on and the external and independent validation dataset pulled from IDC) differ significantly._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1xANs3rKWTo"
      },
      "source": [
        "pred_nrrd_path = os.path.join(pat_dir_path_nrrd, pat + \"_pred_segthor.nrrd\")\n",
        "\n",
        "\n",
        "voxel_spacing = list(sitk_ct.GetSpacing())\n",
        "\n",
        "dc_heart = dice_coef(pred_nrrd_heart, rt_nrrd_heart)\n",
        "hd_heart = hausdorff_distance(pred_nrrd_heart, rt_nrrd_heart, voxelspacing = voxel_spacing)\n",
        "asd_heart = avg_surf_distance(pred_nrrd_heart, rt_nrrd_heart, voxelspacing = voxel_spacing)\n",
        "\n",
        "print(\"Heart Dice Coefficient (raw segmentation) :\", dc_heart)\n",
        "print(\"Heart Hausdorff Distance (raw segmentation) [mm]:\", hd_heart)\n",
        "print(\"Heart Average Surface Distance (raw segmentation) [mm]:\", asd_heart)\n",
        "\n",
        "\n",
        "# run this if and only if a manual esophagus segmentation mask is available\n",
        "if np.sum(rt_nrrd_esophagus):\n",
        "  dc_esophagus = dice_coef(pred_nrrd_esophagus, rt_nrrd_esophagus)\n",
        "  hd_esophagus = hausdorff_distance(pred_nrrd_esophagus, rt_nrrd_esophagus, voxelspacing = voxel_spacing)\n",
        "  asd_esophagus = avg_surf_distance(pred_nrrd_esophagus, rt_nrrd_esophagus, voxelspacing = voxel_spacing)\n",
        "\n",
        "  print(\"\\nEsophagus Dice Coefficient (raw segmentation) :\", dc_esophagus)\n",
        "  print(\"Esophagus Hausdorff Distance (raw segmentation) [mm]:\", hd_esophagus)\n",
        "  print(\"Esophagus Average Surface Distance (raw segmentation) [mm]:\", asd_esophagus)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ehkQblEkgxe"
      },
      "source": [
        "## Finding more relevant images to test\n",
        "\n",
        "Let's get some better idea of the CT series we segmented. We can query IDC BigQuery DICOM metadata table to retrieve some relevant attributes.\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  DISTINCT(Manufacturer),\n",
        "  ManufacturerModelName\n",
        "FROM\n",
        " `bigquery-public-data.idc_current.dicom_all`\n",
        "WHERE\n",
        " SeriesInstanceUID = <my_SeriesInstanceUID>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7-hRJBd05It"
      },
      "source": [
        "details_selection_query = f\"\\\n",
        "  SELECT \\\n",
        "    DISTINCT(Manufacturer), \\\n",
        "    ManufacturerModelName, \\\n",
        "    BodyPartExamined \\\n",
        "  FROM \\\n",
        "    `bigquery-public-data.idc_current.dicom_all` \\\n",
        "  WHERE \\\n",
        "    SeriesInstanceUID = \\\"{my_SeriesInstanceUID}\\\"\"\n",
        "\n",
        "details_selection_result = bq_client.query(details_selection_query)\n",
        "details_selection_df = details_selection_result.result().to_dataframe()\n",
        "\n",
        "details_selection_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYvs4Q6A1SsF"
      },
      "source": [
        "We can easily check what other combinations of `Manufacturer` and `ManufacturerModelName` we have in store, to test generalizability of the segmentation network we have been using. In the query below, we will get all unique combinations of `Manufacturer`/`ManufacturerModelName`, and count how many CT studies that have `BodyPartExamined = LUNG` are available for that specific combination.\n",
        "\n",
        "In a similar fashion, you can utilize any of the DICOM attributes to select representative data to use in testing.\n",
        "\n",
        "Note that such explorations can also be done using IDC portal or IDC DataStudio dashboard. Exploratory SQL queries are best done in the [BigQuery console](https://console.cloud.google.com/bigquery).\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  Manufacturer,\n",
        "  ARRAY_TO_STRING(ARRAY_AGG(DISTINCT(ManufacturerModelName)),','),\n",
        "  COUNT(DISTINCT(StudyInstanceUID)) AS number_of_studies\n",
        "FROM\n",
        "  `bigquery-public-data.idc_current.dicom_all`\n",
        "WHERE\n",
        "  Modality = \"CT\"\n",
        "  AND BodyPartExamined = \"LUNG\"\n",
        "GROUP BY\n",
        "  Manufacturer\n",
        "ORDER BY\n",
        "  number_of_studies DESC\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My6L8D1k2Aoq"
      },
      "source": [
        "sample_selection_query = f\"\\\n",
        "SELECT \\\n",
        "  Manufacturer, \\\n",
        "  ARRAY_TO_STRING(ARRAY_AGG(DISTINCT(ManufacturerModelName)),',') as ManufacturerModelNames, \\\n",
        "  COUNT(DISTINCT(StudyInstanceUID)) AS number_of_studies \\\n",
        "FROM \\\n",
        "  `bigquery-public-data.idc_current.dicom_all` \\\n",
        "WHERE \\\n",
        "  Modality = \\\"CT\\\" \\\n",
        "  AND BodyPartExamined = \\\"LUNG\\\" \\\n",
        "GROUP BY \\\n",
        "  Manufacturer \\\n",
        "ORDER BY \\\n",
        "  number_of_studies DESC\"\n",
        "\n",
        "sample_selection_result = bq_client.query(sample_selection_query)\n",
        "sample_selection_df = sample_selection_result.result().to_dataframe()\n",
        "\n",
        "sample_selection_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOMn1YcH2iu5"
      },
      "source": [
        "Next we can query for a study that has `Manufacturer`/`ManufacturerModelName` combination we want to test, and visualize that study. \n",
        "\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  SeriesInstanceUID,\n",
        "  ARRAY_TO_STRING(ARRAY_AGG(DISTINCT(SeriesDescription)),',') AS SeriesDescriptions,\n",
        "  ARRAY_TO_STRING(ARRAY_AGG(DISTINCT(StudyInstanceUID)),',') AS StudyInstanceUIDs,  \n",
        "  COUNT(DISTINCT(SOPInstanceUID)) AS numberOfInstances\n",
        "FROM\n",
        "  `bigquery-public-data.idc_current.dicom_all`\n",
        "WHERE\n",
        "  Modality = \"CT\"\n",
        "  AND BodyPartExamined = \"LUNG\"\n",
        "  AND Manufacturer = \"GE MEDICAL SYSTEMS\"\n",
        "  AND ManufacturerModelName = \"LightSpeed Xtra\"\n",
        "GROUP BY\n",
        "  SeriesInstanceUID\n",
        "```\n",
        "\n",
        "This query returns three `SeriesInstanceUID`s.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gd4RSiT53zV"
      },
      "source": [
        "print(get_idc_viewer_url(\"1.3.6.1.4.1.14519.5.2.1.3023.4012.146358232297157912733174589662\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKky12H46oaU"
      },
      "source": [
        "Let's re-initialize `SeriesInstanceUID` with the series from this study, and re-run the segmentation steps by returning to the **Visualization and download of data from IDC** section of the notebook after running the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxYJGUvC6rFI"
      },
      "source": [
        "my_SeriesInstanceUID = \"1.3.6.1.4.1.14519.5.2.1.3023.4012.162275549801143329076803363880\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUVGRNxZQerC"
      },
      "source": [
        "## I want to train my network, not run inference - what do I do?\n",
        "\n",
        "Google Colab has a number of limitations on the resources available and time you can use this free resource. If you plan to train deep learning models, you should consider using GCP [Vertex AI Notebooks](https://cloud.google.com/vertex-ai-workbench), which is a paid resource that you can use to define highly configurable AI development environments accessible via JupyterLab interface. You can [apply for a sponsored project from IDC](https://learn.canceridc.dev/introduction/requesting-gcp-cloud-credits) to experiment with those capabilities at no cost to you."
      ]
    }
  ]
}